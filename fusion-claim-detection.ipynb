{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/working-files/annotations\")\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import os\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from io import BytesIO\n",
    "import captum\n",
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import NoiseTunnel\n",
    "#from torchsummary import summary\n",
    "import wget\n",
    "import sqlite3, json, io\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from pprint import pprint\n",
    "import json\n",
    "import math\n",
    "from math import sqrt\n",
    "import torch\n",
    "import itertools \n",
    "import torchtext\n",
    "from torchtext.data import Field, Dataset, Example\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from google.cloud import vision\n",
    "import sys,os,json\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from google.cloud import vision\n",
    "from google.protobuf.json_format import MessageToJson\n",
    "import requests\n",
    "import logging\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "from torch.utils.data import DataLoader\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import io\n",
    "from io import BytesIO\n",
    "from itertools import chain\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_df(annotation_json):\n",
    "    data = pd.read_json(annotation_json)\n",
    "    if \"annotation\" not in data.columns:\n",
    "        df = pd.DataFrame(data.T[\"metadata\"].tolist()).merge(pd.DataFrame(data.T[\"annotation\"].tolist()))\n",
    "    else:\n",
    "        df = pd.DataFrame(data[\"metadata\"].tolist()).merge(pd.DataFrame(data[\"annotation\"].tolist()))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind1 = construct_df(os.environ.get(\"ANNOTATION_IND1\"))\n",
    "ind2 = construct_df(os.environ.get(\"ANNOTATION_IND2\")\n",
    "ind3 = construct_df(os.environ.get(\"ANNOTATION_IND3\")\n",
    "ind4 = construct_df(os.environ.get(\"ANNOTATION_IND4\")\n",
    "common1 = construct_df(os.environ.get(\"ANNOTATION_COMMON1\")\n",
    "common2 = construct_df(os.environ.get(\"ANNOTATION_COMMON2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = common1.append(common2).append(ind1).append(ind2).append(ind3).append(ind4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_id', 'media_type', 'post_permalink', 'tag_name', 's3_url',\n",
       "       'timestamp', 'caption', 'text', 'field-one', 'field-two', 'field-three',\n",
       "       'field-four', 'field-five', 'field-six', 'field-seven',\n",
       "       'annotator_notes', 'has_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\"field-one\":\"verifiable_claim\", \"field-two\":\"unidentified_video\", \"field-three\":\"contains_images\",\n",
    "                  \"field-four\":\"visible_source\", \"field-five\":\"meme\", \"field-six\":\"solicitations\", \"field-seven\":\"worth_archiving\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"sharechat_annotated_9092020.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1363\n"
     ]
    }
   ],
   "source": [
    "images_df=df[df[\"media_type\"]==\"image\"]\n",
    "print(len(images_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"working-files/annotated-images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_id', 'media_type', 'post_permalink', 'tag_name', 's3_url',\n",
       "       'timestamp', 'caption', 'text', 'verifiable_claim',\n",
       "       'unidentified_video', 'contains_images', 'visible_source', 'meme',\n",
       "       'solicitations', 'worth_archiving', 'annotator_notes', 'has_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for i in images_df[\"s3_url\"]:\n",
    "#     wget.download(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def claims(data):\n",
    "    if \"Yes\" in str(data):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_df[\"claim\"] = images_df[\"verifiable_claim\"].map(claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verifiable_claim</th>\n",
       "      <th>claim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Yes, Descriptions of a real world events/plac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[No ]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[No ]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[Yes, Descriptions of a real world events/plac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[Yes, Descriptions of a real world events/plac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    verifiable_claim  claim\n",
       "0  [Yes, Descriptions of a real world events/plac...      1\n",
       "3                                              [No ]      0\n",
       "4                                              [No ]      0\n",
       "7  [Yes, Descriptions of a real world events/plac...      1\n",
       "8  [Yes, Descriptions of a real world events/plac...      1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_df[[\"verifiable_claim\",\"claim\"]][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download word embeddings and extract text from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wget.download(\"https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.hi.align.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_text(path, client):  \n",
    "    def detect_text(img_bytes, client):\n",
    "        image_data = vision.types.Image(content=img_bytes)\n",
    "        resp = client.text_detection(image=image_data)\n",
    "        resp = json.loads(MessageToJson(resp))\n",
    "        text = resp.get('fullTextAnnotation',{}).get(\"text\",\"\")\n",
    "        return text\n",
    "    image = Image.open(path, mode=\"r\")\n",
    "    imgByteArr = io.BytesIO()\n",
    "    image.save(imgByteArr, format=\"PNG\")\n",
    "    img_bytes = imgByteArr.getvalue()\n",
    "    text = detect_text(img_bytes, client)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# f=0\n",
    "# extractions = {}\n",
    "# texts = []\n",
    "# client = vision.ImageAnnotatorClient()\n",
    "# img_folder = os.getcwd()\n",
    "# for i in os.listdir(img_folder):\n",
    "#     try:\n",
    "#         text = get_image_text(i, client)\n",
    "#         extractions[i] = text\n",
    "#         texts.append(text)\n",
    "#     except:\n",
    "#         f+=1\n",
    "#         extractions[i] = \"\"\n",
    "#         texts.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"extractions.json\",\"w\") as f:\n",
    "#     json.dump(extractions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../extractions.json\",\"r\") as f:\n",
    "    extractions = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_texts(url):\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    text = extractions.get(filename)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename(url):\n",
    "    return url.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_df[\"extracted_text\"] = images_df[\"s3_url\"].map(match_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2191     https://sharechat.com/post/pAvG5KK\n",
       "2192     https://sharechat.com/post/8Vvgx0V\n",
       "2193     https://sharechat.com/post/8nd1zAw\n",
       "2194     https://sharechat.com/post/xn9e3vw\n",
       "2197    https://sharechat.com/post/8rZ4Zbzj\n",
       "Name: post_permalink, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_df.tail(5)[\"post_permalink\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2191    मृत्यु भोज समाज के लिये अभिशाप : जगतगुरु\\nने स...\n",
       "2192    share plz\\nall group\\nअगर किसी की दोनो किउनिया...\n",
       "2193                                      HindiSoch.Com\\n\n",
       "2194    LIFES 6 RULES FOR SUCCESS:\\nI. TRUST YOURSELF\\...\n",
       "2197    All Problem Solution Specialist Astrologer\\n+9...\n",
       "Name: extracted_text, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_df.tail(5)[\"extracted_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    802\n",
       "1    561\n",
       "Name: claim, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_df[\"claim\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_df[\"filename\"] = images_df[\"s3_url\"].map(get_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_df = images_df[[\"filename\", \"extracted_text\", \"claim\"]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_df.rename(columns={\"filename\":\"img_name\", \"extracted_text\":\"text\",\"claim\":\"label\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images, labels and image text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations=transforms.Compose([\n",
    "                    transforms.Resize((224,224)), \n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir, texts_and_labels, transform):\n",
    "        if isinstance(texts_and_labels, pd.core.frame.DataFrame):\n",
    "            df = texts_and_labels\n",
    "        else:\n",
    "            # initial model training data was loaded this way\n",
    "            with open(texts_and_labels, \"r\") as fp:\n",
    "                data = json.load(fp)\n",
    "            df = pd.DataFrame.from_dict(data, orient=\"index\").reset_index()\n",
    "            df.columns = [\"img_name\", \"text\"]\n",
    "            df[\"label\"] = df[\"img_name\"].apply(lambda x: 0 if \"neg\" in x else 1)\n",
    "        self.img_dir=img_dir\n",
    "        self.img_names=df[\"img_name\"].values\n",
    "        self.text = df[\"text\"].values\n",
    "        self.y = df[\"label\"].values\n",
    "        self.transform=transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # For each img_name in img_names, load the corresponding img from the img folder\n",
    "        img = Image.open(os.path.join(self.img_dir, self.img_names[idx])).convert('RGB')\n",
    "        # Return the transformed RGB image\n",
    "        if self.transform is not None:\n",
    "            img_vector=self.transform(img)\n",
    "        \n",
    "        # Return the corresponding label and text\n",
    "        label = self.y[idx]\n",
    "        text = self.text[idx]\n",
    "        return img_vector,label,text\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(\n",
    "                       img_dir=os.environ.get(\"IMAGE_FOLDER\"),\n",
    "                       texts_and_labels=claims_df,\n",
    "                       transform=transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create text preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path):\n",
    "    \"\"\" Loads pretrained aligned word embeddings \"\"\"\n",
    "    print(\"Loading pretrained aligned word embeddings\")\n",
    "    words = []\n",
    "    idx = 0\n",
    "    word2idx = {}\n",
    "    vectors = []\n",
    "    fin = io.open(path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    for line in tqdm.tqdm(fin,total=n):\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        vec = (list(map(float, tokens[1:])))\n",
    "        word = tokens[0].replace(\"'\",'\"')\n",
    "        words.append(word)\n",
    "        word2idx[word]=idx\n",
    "        vectors.append(vec)\n",
    "        idx+=1\n",
    "    print(\"Number of embeddings loaded: \", len(vectors))\n",
    "    vectors = np.array(vectors)\n",
    "    aligned_dict = {w: vectors[word2idx[w]] for w in words}\n",
    "    return aligned_dict\n",
    "\n",
    "def prepare_sequences(dataloader, inference=False, pretrained=None, vocab2index=None):\n",
    "    \"\"\" Encodes text into sequences of vocabulary indices \"\"\"\n",
    "    \n",
    "    def tokenize(text):\n",
    "        \"\"\" Removes punctuation and numbers, converts to lowercase and splits into individual words \"\"\"\n",
    "        regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n\\।]') # last character । is the Hindi full stop\n",
    "        nopunct = regex.sub(\" \", text.lower())\n",
    "        tokenized_text = [token for token in nopunct.split(\" \") if len(token) > 0]\n",
    "        return tokenized_text\n",
    "    \n",
    "    def count_words(tokens, counts):\n",
    "        \"\"\" Counts unique words in text \"\"\"\n",
    "        counts.update(list(chain.from_iterable(tokens)))\n",
    "        return counts\n",
    "    \n",
    "    def get_emb_matrix(pretrained, word_counts, emb_size = 300):\n",
    "        \"\"\" Creates training vocabulary, vocab2index and embedding matrix from pretrained word vectors \"\"\"\n",
    "        found=0\n",
    "        not_found=0\n",
    "        vocab_size = len(word_counts) + 2\n",
    "        vocab_to_idx = {}\n",
    "        vocab = [\"\", \"UNK\"]\n",
    "        W = np.zeros((vocab_size, emb_size), dtype=\"float32\")\n",
    "        W[0] = np.zeros(emb_size, dtype='float32') # adding a vector for padding\n",
    "        W[1] = np.random.uniform(-0.25, 0.25, emb_size) # adding a vector for unknown words \n",
    "        vocab_to_idx[\"UNK\"] = 1\n",
    "        i = 2\n",
    "        for word in word_counts:\n",
    "            if word in pretrained:\n",
    "                W[i] = pretrained[word]\n",
    "                found+=1\n",
    "            else:\n",
    "                W[i] = np.random.uniform(-0.25,0.25, emb_size)\n",
    "                not_found+=1\n",
    "            vocab_to_idx[word] = i\n",
    "            vocab.append(word)\n",
    "            i += 1   \n",
    "        return W, np.array(vocab), vocab_to_idx, found, not_found\n",
    "    \n",
    "    def encode_tokens(tokenized_text, vocab2index, N=50): # keep max doc length to 50 words\n",
    "        \"\"\" Encodes tokenized text into equal length sequences of vocabulary indices \"\"\" \n",
    "        sequences = []\n",
    "        for text in tokenized_text:\n",
    "            seq = np.zeros(N, dtype=int)\n",
    "            # get word index if it's in vocab else get default index\n",
    "            enc = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in text])\n",
    "            # limit sequence length\n",
    "            length = min(N, len(enc))\n",
    "            seq[:length] = enc[:length]\n",
    "            sequences.append(seq)\n",
    "        return sequences\n",
    "\n",
    "    if inference==False:\n",
    "        counts = Counter()\n",
    "        tokenized_text = []\n",
    "        print(\"Tokenizing text and counting unique words\")\n",
    "        for i in dataloader:\n",
    "            tokenized = [tokenize(doc) for doc in i[2]]\n",
    "            counts = count_words(tokenized, counts)\n",
    "            tokenized_text.extend(tokenized)  \n",
    "\n",
    "        print(\"Creating vocab2index and embedding matrix with pretrained weights\")\n",
    "        pretrained_weights, vocab, vocab2index, found, not_found = get_emb_matrix(pretrained, counts)\n",
    "        sequences = encode_tokens(tokenized_text, vocab2index)\n",
    "        vocab_size = len(vocab)\n",
    "        print(\"Finished preparing sequences\")\n",
    "        return sequences, vocab, vocab_size, vocab2index, pretrained_weights\n",
    "    else:\n",
    "        print(\"Tokenizing text\")\n",
    "        tokenized_text = []\n",
    "        for i in dataloader:\n",
    "            tokenized = [tokenize(doc) for doc in i[2]]\n",
    "            tokenized_text.extend(tokenized)\n",
    "        sequences = encode_tokens(tokenized_text, vocab2index)\n",
    "        print(\"Finished preparing sequences\")\n",
    "        return sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and randomly split dataset into 80:20 training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "valid_size = len(dataset) - train_size\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=False)\n",
    "validation_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.environ.get(\"EMBEDDINGS_PATH\")\n",
    "aligned_dict = load_embeddings(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 786/158016 [00:00<00:20, 7856.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained aligned word embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 158016/158016 [00:16<00:00, 9783.35it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings loaded:  158016\n",
      "Tokenizing text and counting unique words\n",
      "Creating vocab2index and embedding matrix with pretrained weights\n",
      "Finished preparing sequences\n"
     ]
    }
   ],
   "source": [
    "train_sequences, vocab, vocab_size, vocab2index, pretrained_weights = prepare_sequences(train_loader, inference=False, pretrained=aligned_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing text\n",
      "Finished preparing sequences\n"
     ]
    }
   ],
   "source": [
    "valid_sequences = prepare_sequences(validation_loader, inference=True, vocab2index=vocab2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_loader = torch.utils.data.DataLoader(train_sequences, batch_size=8, shuffle=False)\n",
    "valid_seq_loader = torch.utils.data.DataLoader(valid_sequences, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, v in vocab2index.items():\n",
    "#     if v==102:\n",
    "#         print (k)\n",
    "\n",
    "# vocab2index[\"कर\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, pretrained_weights, num_labels):\n",
    "        super().__init__()\n",
    "        # LSTM layers\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embeddings.weight.data.copy_(torch.from_numpy(pretrained_weights))\n",
    "        self.embeddings.weight.requires_grad = False ## freeze embeddings\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 100)\n",
    "        #self.dropout = nn.Dropout(0.2)\n",
    "        # ResNet layers\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Replace the default ResNet18 classifier layer \n",
    "        classifier_input = self.resnet.fc.in_features\n",
    "        classifier = nn.Sequential(nn.Linear(classifier_input, 100), \n",
    "                             nn.LogSoftmax(dim=1)) \n",
    "        self.resnet.fc = classifier\n",
    "        self.fusion_classifier = nn.Linear(200,2)\n",
    "        \n",
    "    def forward(self, x1, x2): # image vector, encoded image text\n",
    "#         print(\"x1:\",x1)\n",
    "#         print(\"x2\",x2)\n",
    "        # Image\n",
    "        x1_out = self.resnet(x1)\n",
    "        # Text\n",
    "        x2_embed = self.embeddings(x2)\n",
    "        #x2_dropout = self.dropout(x2_embed)\n",
    "#         print(\"img representation:\",x1_out)\n",
    "#         print(\"\")\n",
    "        lstm_out, (ht, ct) = self.lstm(x2_embed)\n",
    "        x2_out = self.linear(ht[-1])\n",
    "#         print(\"text representation:\",x2_out)\n",
    "#         print(\"\")\n",
    "        # Concatenation\n",
    "        fused = torch.cat([x2_out, x1_out], dim=1)\n",
    "#         print(\"fused:\", fused)\n",
    "\n",
    "        return self.fusion_classifier(fused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FusionModel(vocab_size=vocab_size, embedding_dim=300, hidden_dim=5, pretrained_weights=pretrained_weights,\n",
    "                    num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs, lr, train_loader, train_seq_loader):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    model.train()\n",
    "    for i in range(epochs):\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(zip(train_loader, train_seq_loader)):\n",
    "        #for data in zip(validation_loader, valid_seq_loader): \n",
    "            img_vectors, text_sequences, y_train = data[0][0], data[1], data[0][1]      \n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(img_vectors, text_sequences)\n",
    "            loss = F.cross_entropy(y_hat, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y_train.shape[0]\n",
    "            total += y_train.shape[0]\n",
    "            pred = torch.max(y_hat, 1)[1]\n",
    "            correct += (pred == y_train).float().sum()\n",
    "        val_loss, val_acc = validation_metrics(model, validation_loader, valid_seq_loader)\n",
    "        print(\"train loss %.3f, train accuracy %.3f, val loss %.3f, val accuracy %.3f\" % (sum_loss/total, correct/total, val_loss, val_acc))\n",
    "\n",
    "def validation_metrics (model, validation_loader=validation_loader, valid_seq_loader=valid_seq_loader):\n",
    "    # Initialize the prediction and label lists(tensors)\n",
    "    predlist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "    lbllist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    for i, data in enumerate(zip(validation_loader, valid_seq_loader)):\n",
    "        #\n",
    "        img_vectors, text_sequences, y_valid = data[0][0], data[1], data[0][1] \n",
    "        y_hat = model(img_vectors, text_sequences)\n",
    "        loss = F.cross_entropy(y_hat, y_valid)\n",
    "        #print(\"y_hat:\",y_hat)\n",
    "        pred = torch.max(y_hat, 1)[1]\n",
    "        #print(\"pred:\",pred)\n",
    "        # Append batch prediction results\n",
    "        predlist=torch.cat([predlist,pred.view(-1).cpu()])\n",
    "        lbllist=torch.cat([lbllist,y_valid.view(-1).cpu()])\n",
    "        # Calculate accuracy\n",
    "        correct += (pred == y_valid).float().sum()\n",
    "        total += y_valid.shape[0]\n",
    "        sum_loss += loss.item()*y_valid.shape[0]\n",
    "    \n",
    "    # Confusion matrix\n",
    "    conf_mat=confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
    "    print(conf_mat)\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    class_accuracy=100*conf_mat.diagonal()/conf_mat.sum(1)\n",
    "    print(class_accuracy)\n",
    "    \n",
    "    return sum_loss/total, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 74  86]\n",
      " [  9 104]]\n",
      "[46.25       92.03539823]\n",
      "train loss 0.561, train accuracy 0.732, val loss 0.658, val accuracy 0.652\n",
      "[[ 96  64]\n",
      " [ 12 101]]\n",
      "[60.         89.38053097]\n",
      "train loss 0.451, train accuracy 0.794, val loss 0.574, val accuracy 0.722\n",
      "[[101  59]\n",
      " [ 13 100]]\n",
      "[63.125      88.49557522]\n",
      "train loss 0.412, train accuracy 0.812, val loss 0.554, val accuracy 0.736\n",
      "[[ 99  61]\n",
      " [ 11 102]]\n",
      "[61.875      90.26548673]\n",
      "train loss 0.382, train accuracy 0.829, val loss 0.561, val accuracy 0.736\n",
      "[[ 90  70]\n",
      " [ 11 102]]\n",
      "[56.25       90.26548673]\n",
      "train loss 0.359, train accuracy 0.841, val loss 0.605, val accuracy 0.703\n",
      "[[117  43]\n",
      " [ 18  95]]\n",
      "[73.125      84.07079646]\n",
      "train loss 0.324, train accuracy 0.848, val loss 0.526, val accuracy 0.777\n",
      "[[ 97  63]\n",
      " [ 12 101]]\n",
      "[60.625      89.38053097]\n",
      "train loss 0.301, train accuracy 0.875, val loss 0.609, val accuracy 0.725\n",
      "[[116  44]\n",
      " [ 17  96]]\n",
      "[72.5        84.95575221]\n",
      "train loss 0.275, train accuracy 0.880, val loss 0.537, val accuracy 0.777\n",
      "[[111  49]\n",
      " [ 14  99]]\n",
      "[69.375      87.61061947]\n",
      "train loss 0.248, train accuracy 0.900, val loss 0.578, val accuracy 0.769\n",
      "[[120  40]\n",
      " [ 16  97]]\n",
      "[75.         85.84070796]\n",
      "train loss 0.223, train accuracy 0.909, val loss 0.545, val accuracy 0.795\n",
      "CPU times: user 27min 47s, sys: 2min 33s, total: 30min 20s\n",
      "Wall time: 24min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_model(model=model, epochs=10, lr=0.001, train_loader=train_loader, train_seq_loader=train_seq_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1595852529951,
   "trusted": true
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
