{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression analysis - Sharechat annotated claims dataset\n",
    "\n",
    "Research - https://arxiv.org/abs/2010.13387"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/working-files\")\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from pprint import pprint\n",
    "import json\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import pandas as pd\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "pd.options.mode.chained_assignment = None\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier, DMatrix\n",
    "from xgboost import to_graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"working-files/sharechat_annotated_transformed.csv\", index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that won't be required\n",
    "drop_cols = [\n",
    "        \"tag_name\", \"tag_translation\", \"timestamp\", \n",
    "        \"caption\", \"text\", \"filename\",\n",
    "        \"datetime\", \"extracted_text\", \"named_entities\", \"combined_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X = df.drop(\"claim\", axis=1)\n",
    "y = df[\"claim\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    # Drop columns that won't be used \n",
    "    df.drop(drop_cols, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = prepare_data(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize numeric variables\n",
    "scaler = StandardScaler()\n",
    "num = [\"n_words\", \"n_hashtags\", \"likes\", \"external_shares\", \"entities_count\"]\n",
    "num_cols = pd.DataFrame(scaler.fit_transform(X_train[num]))\n",
    "num_cols.columns = num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical columns\n",
    "cat = [\n",
    "    \"media_type\", \"contains_video\", \"contains_image\", \n",
    "    \"contains_relevant_meme\", \"bucket_name\", \"hour\"]\n",
    "X_train[cat] = X_train[cat].apply(pd.Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummify categorical columns\n",
    "cat_cols = pd.get_dummies(X_train[cat], drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([cat_cols, num_cols.set_index(cat_cols.index)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to test set\n",
    "X_test = prepare_data(X_test)\n",
    "\n",
    "num_cols = pd.DataFrame(scaler.transform(X_test[num]))\n",
    "num_cols.columns = num\n",
    "\n",
    "X_test[cat] = X_test[cat].apply(pd.Categorical)\n",
    "\n",
    "cat_cols = pd.get_dummies(X_test[cat], drop_first = True)\n",
    "X_test = pd.concat([cat_cols, num_cols.set_index(cat_cols.index)], axis=1)\n",
    "\n",
    "y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to fix column difference between train and test sets\n",
    "\n",
    "# This will go inside the main function\n",
    "def add_missing_dummy_columns(test_set, train_columns):\n",
    "    # d = test set, columns = train set columns \n",
    "    missing_cols = set(train_columns) - set(test_set.columns)\n",
    "    for c in missing_cols:\n",
    "        test_set[c] = 0 # add missing columns to test set with empty column values\n",
    "        \n",
    "# This is the main function     \n",
    "def fix_columns(test_set, train_columns):  \n",
    "\n",
    "    add_missing_dummy_columns(test_set, train_columns)\n",
    "\n",
    "    # make sure we have all the columns we need\n",
    "    assert(set(train_columns) - set(test_set.columns) == set())\n",
    "\n",
    "    extra_cols = set(test_set.columns) - set(train_columns) # these are the extra cols in the test set\n",
    "    if extra_cols:\n",
    "        print (\"extra columns:\", extra_cols)\n",
    "\n",
    "    test_set = test_set[train_columns] # keep only columns that are in the train set \n",
    "    return test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extra columns: {'bucket_name_राजनीति '}\n"
     ]
    }
   ],
   "source": [
    "X_test = fix_columns(X_test, X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "assert(set(X_train.columns) - set(X_test.columns) == set())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9852272727272727\n",
      "0.7613636363636364\n"
     ]
    }
   ],
   "source": [
    "clf = XGBClassifier(random_state = 0)\n",
    "clf.fit(X_train, y_train)\n",
    "pred_train = clf.predict(X_train)\n",
    "pred_test = clf.predict(X_test) \n",
    "print(clf.score(X_train, y_train))\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(random_state = 0)\n",
    "params = {\n",
    "    \"max_depth\": list(range(2,9)),\n",
    "    \"min_child_weight\": list(range(1,11)),\n",
    "    \"subsample\": [0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
    "    \"colsample_bytree\": [0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
    "    \"eta\": [0.05, 0.1, 0.2, 0.3],\n",
    "    \"gamma\": list(range(0, 11))\n",
    "    \n",
    "}\n",
    "clf = RandomizedSearchCV(estimator=xgb, param_distributions=params, n_iter=999, scoring=\"accuracy\", n_jobs=-1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None, gamma=None,\n",
       "                                           gpu_id=None, importance_type='gain',\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constraints=None,\n",
       "                                           n_estimators=100, n_job...\n",
       "                                           subsample=None, tree_method=None,\n",
       "                                           validate_parameters=None,\n",
       "                                           verbosity=None),\n",
       "                   n_iter=999, n_jobs=-1,\n",
       "                   param_distributions={'colsample_bytree': [0.5, 0.6, 0.7, 0.8,\n",
       "                                                             0.9, 1],\n",
       "                                        'eta': [0.05, 0.1, 0.2, 0.3],\n",
       "                                        'gamma': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9,\n",
       "                                                  10],\n",
       "                                        'max_depth': [2, 3, 4, 5, 6, 7, 8],\n",
       "                                        'min_child_weight': [1, 2, 3, 4, 5, 6,\n",
       "                                                             7, 8, 9, 10],\n",
       "                                        'subsample': [0.5, 0.6, 0.7, 0.8, 0.9,\n",
       "                                                      1]},\n",
       "                   random_state=0, scoring='accuracy')"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8886363636363637\n",
      "0.7272727272727273\n"
     ]
    }
   ],
   "source": [
    "pred_train = clf.predict(X_train)\n",
    "pred_test = clf.predict(X_test) \n",
    "print(clf.score(X_train, y_train))\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['working-files/randomsearch-xgb-best.joblib']"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf, 'working-files/randomsearch-xgb.pkl')\n",
    "joblib.dump(clf, 'working-files/randomsearch-xgb.joblib')\n",
    "joblib.dump(clf.best_estimator_, 'working-files/randomsearch-xgb-best.pkl')\n",
    "joblib.dump(clf.best_estimator_, 'working-files/randomsearch-xgb-best.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = joblib.load('working-files/gridsearch-xgb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = DMatrix(X_train, y_train)\n",
    "dtest = DMatrix(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default parameters\n",
    "params = {\n",
    "    'max_depth':6,\n",
    "    'min_child_weight': 1,\n",
    "    'eta':.3,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    \"eval_metric\": \"auc\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tTest-auc:0.73007\n",
      "Will train until Test-auc hasn't improved in 10 rounds.\n",
      "[1]\tTest-auc:0.75932\n",
      "[2]\tTest-auc:0.77986\n",
      "[3]\tTest-auc:0.78844\n",
      "[4]\tTest-auc:0.79142\n",
      "[5]\tTest-auc:0.78563\n",
      "[6]\tTest-auc:0.78745\n",
      "[7]\tTest-auc:0.78921\n",
      "[8]\tTest-auc:0.79183\n",
      "[9]\tTest-auc:0.79345\n",
      "[10]\tTest-auc:0.79427\n",
      "[11]\tTest-auc:0.79322\n",
      "[12]\tTest-auc:0.79210\n",
      "[13]\tTest-auc:0.79379\n",
      "[14]\tTest-auc:0.79321\n",
      "[15]\tTest-auc:0.79346\n",
      "[16]\tTest-auc:0.79707\n",
      "[17]\tTest-auc:0.80260\n",
      "[18]\tTest-auc:0.80469\n",
      "[19]\tTest-auc:0.80458\n",
      "[20]\tTest-auc:0.80360\n",
      "[21]\tTest-auc:0.80550\n",
      "[22]\tTest-auc:0.80490\n",
      "[23]\tTest-auc:0.80663\n",
      "[24]\tTest-auc:0.80170\n",
      "[25]\tTest-auc:0.80091\n",
      "[26]\tTest-auc:0.80273\n",
      "[27]\tTest-auc:0.80070\n",
      "[28]\tTest-auc:0.80036\n",
      "[29]\tTest-auc:0.80162\n",
      "[30]\tTest-auc:0.80409\n",
      "[31]\tTest-auc:0.80484\n",
      "[32]\tTest-auc:0.80561\n",
      "[33]\tTest-auc:0.80663\n",
      "Stopping. Best iteration:\n",
      "[23]\tTest-auc:0.80663\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = xgboost.train(params=params, dtrain=dtrain, num_boost_round=999, evals = [(dtest, \"Test\")], early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-auc-mean</th>\n",
       "      <th>train-auc-std</th>\n",
       "      <th>test-auc-mean</th>\n",
       "      <th>test-auc-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.843357</td>\n",
       "      <td>0.005258</td>\n",
       "      <td>0.738208</td>\n",
       "      <td>0.014341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.881911</td>\n",
       "      <td>0.006186</td>\n",
       "      <td>0.764922</td>\n",
       "      <td>0.018025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.895542</td>\n",
       "      <td>0.006422</td>\n",
       "      <td>0.774323</td>\n",
       "      <td>0.020917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.908203</td>\n",
       "      <td>0.009030</td>\n",
       "      <td>0.777084</td>\n",
       "      <td>0.019125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.919838</td>\n",
       "      <td>0.008381</td>\n",
       "      <td>0.783305</td>\n",
       "      <td>0.018396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.929269</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>0.780366</td>\n",
       "      <td>0.018157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.938783</td>\n",
       "      <td>0.008875</td>\n",
       "      <td>0.781640</td>\n",
       "      <td>0.016003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.945691</td>\n",
       "      <td>0.007295</td>\n",
       "      <td>0.784530</td>\n",
       "      <td>0.016583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.949663</td>\n",
       "      <td>0.006850</td>\n",
       "      <td>0.785485</td>\n",
       "      <td>0.015185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.953918</td>\n",
       "      <td>0.007806</td>\n",
       "      <td>0.786649</td>\n",
       "      <td>0.017125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.957615</td>\n",
       "      <td>0.008903</td>\n",
       "      <td>0.788552</td>\n",
       "      <td>0.016243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.961115</td>\n",
       "      <td>0.007570</td>\n",
       "      <td>0.787551</td>\n",
       "      <td>0.014818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.962785</td>\n",
       "      <td>0.007218</td>\n",
       "      <td>0.787796</td>\n",
       "      <td>0.014449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.966736</td>\n",
       "      <td>0.006797</td>\n",
       "      <td>0.789258</td>\n",
       "      <td>0.014057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train-auc-mean  train-auc-std  test-auc-mean  test-auc-std\n",
       "0         0.843357       0.005258       0.738208      0.014341\n",
       "1         0.881911       0.006186       0.764922      0.018025\n",
       "2         0.895542       0.006422       0.774323      0.020917\n",
       "3         0.908203       0.009030       0.777084      0.019125\n",
       "4         0.919838       0.008381       0.783305      0.018396\n",
       "5         0.929269       0.007519       0.780366      0.018157\n",
       "6         0.938783       0.008875       0.781640      0.016003\n",
       "7         0.945691       0.007295       0.784530      0.016583\n",
       "8         0.949663       0.006850       0.785485      0.015185\n",
       "9         0.953918       0.007806       0.786649      0.017125\n",
       "10        0.957615       0.008903       0.788552      0.016243\n",
       "11        0.961115       0.007570       0.787551      0.014818\n",
       "12        0.962785       0.007218       0.787796      0.014449\n",
       "13        0.966736       0.006797       0.789258      0.014057"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = xgboost.cv(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=999,\n",
    "    seed=42,\n",
    "    nfold=5,\n",
    "    metrics={'auc'},\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1077 4 22 657\n"
     ]
    }
   ],
   "source": [
    "# Count true / false positives / negatives\n",
    "tn, fp, fn, tp = confusion_matrix(y_train, pred_train).ravel()\n",
    "print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220 39 66 115\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, pred_test).ravel()\n",
    "print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      1081\n",
      "           1       0.99      0.97      0.98       679\n",
      "\n",
      "    accuracy                           0.99      1760\n",
      "   macro avg       0.99      0.98      0.98      1760\n",
      "weighted avg       0.99      0.99      0.99      1760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy metrics\n",
    "print(classification_report(y_train, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.85      0.81       259\n",
      "           1       0.75      0.64      0.69       181\n",
      "\n",
      "    accuracy                           0.76       440\n",
      "   macro avg       0.76      0.74      0.75       440\n",
      "weighted avg       0.76      0.76      0.76       440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1595852529951,
   "trusted": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
