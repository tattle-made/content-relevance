{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import os\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from io import BytesIO\n",
    "import captum\n",
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import NoiseTunnel\n",
    "from torchsummary import summary\n",
    "import wget\n",
    "import sqlite3, json, io\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from pprint import pprint\n",
    "import json\n",
    "import math\n",
    "from math import sqrt\n",
    "import torch\n",
    "import itertools \n",
    "import torchtext\n",
    "from torchtext.data import Field, Dataset, Example\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from google.cloud import vision\n",
    "import sys,os,json\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from google.cloud import vision\n",
    "from google.protobuf.json_format import MessageToJson\n",
    "import requests\n",
    "import logging\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "from torch.utils.data import DataLoader\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import io\n",
    "from io import BytesIO\n",
    "from itertools import chain\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download word embeddings and extract text from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wget.download(\"https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.hi.align.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store\n",
      "CPU times: user 2min 22s, sys: 11.6 s, total: 2min 34s\n",
      "Wall time: 16min 36s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# def get_image_text(path, client):  \n",
    "#     def detect_text(img_bytes, client):\n",
    "#         image_data = vision.types.Image(content=img_bytes)\n",
    "#         resp = client.text_detection(image=image_data)\n",
    "#         resp = json.loads(MessageToJson(resp))\n",
    "#         text = resp.get('fullTextAnnotation',{}).get(\"text\",\"\")\n",
    "#         return text\n",
    "#     image = Image.open(path, mode=\"r\")\n",
    "#     imgByteArr = io.BytesIO()\n",
    "#     image.save(imgByteArr, format=\"PNG\")\n",
    "#     img_bytes = imgByteArr.getvalue()\n",
    "#     text = detect_text(img_bytes, client)\n",
    "#     return text\n",
    "\n",
    "# extractions = {}\n",
    "# client = vision.ImageAnnotatorClient()\n",
    "# img_folder = \"/Users/kruttikanadig/Documents/Tattle/machine-learning/all/\"\n",
    "# for i in os.listdir(img_folder):\n",
    "#     try:\n",
    "#         name = i\n",
    "#         img_path = img_folder+i\n",
    "#         text = get_image_text(img_path, client)\n",
    "#         extractions[name] = text\n",
    "#     except:\n",
    "#         print(i)\n",
    "#         f+=1\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images, labels and image text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations=transforms.Compose([\n",
    "                    transforms.Resize((224,224)), \n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir, texts_and_labels, transform):\n",
    "           \n",
    "        with open(texts_and_labels, \"r\") as fp:\n",
    "            data = json.load(fp)\n",
    "        df = pd.DataFrame.from_dict(data, orient=\"index\").reset_index()\n",
    "        df.columns = [\"img_name\", \"text\"]\n",
    "        df[\"label\"] = df[\"img_name\"].apply(lambda x: 0 if \"neg\" in x else 1)\n",
    "        \n",
    "        self.img_dir=img_dir\n",
    "        self.img_names=df[\"img_name\"].values\n",
    "        self.text = df[\"text\"].values\n",
    "        self.y = df[\"label\"].values\n",
    "        self.transform=transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # For each img_name in img_names, load the corresponding img from the img folder\n",
    "        img = Image.open(os.path.join(self.img_dir, self.img_names[idx])).convert('RGB')\n",
    "        # Return the transformed RGB image\n",
    "        if self.transform is not None:\n",
    "            img_vector=self.transform(img)\n",
    "        \n",
    "        # Return the corresponding label and text\n",
    "        label = self.y[idx]\n",
    "        text = self.text[idx]\n",
    "        return img_vector,label,text\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(\n",
    "                       img_dir=\"/Users/kruttikanadig/Documents/Tattle/content-relevance/images\",\n",
    "                       texts_and_labels=\"extractions.json\",\n",
    "                       transform=transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create text preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(dataloader, inference=False, path=None, vocab2index=None):\n",
    "    \"\"\" Encodes text into sequences of vocabulary indices \"\"\"\n",
    "    \n",
    "    def tokenize(text):\n",
    "        \"\"\" Removes punctuation and numbers, converts to lowercase and splits into individual words \"\"\"\n",
    "        regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n\\ред]') # last character ред is the Hindi full stop\n",
    "        nopunct = regex.sub(\" \", text.lower())\n",
    "        tokenized_text = [token for token in nopunct.split(\" \") if len(token) > 0]\n",
    "        return tokenized_text\n",
    "    \n",
    "    def count_words(tokens, counts):\n",
    "        \"\"\" Counts unique words in text \"\"\"\n",
    "        counts.update(list(chain.from_iterable(tokens)))\n",
    "        return counts\n",
    "    \n",
    "    def load_embeddings(path):\n",
    "        \"\"\" Loads pretrained aligned word embeddings \"\"\"\n",
    "        words = []\n",
    "        idx = 0\n",
    "        word2idx = {}\n",
    "        vectors = []\n",
    "        fin = io.open(path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "        n, d = map(int, fin.readline().split())\n",
    "        for line in tqdm.tqdm(fin,total=n):\n",
    "            tokens = line.rstrip().split(' ')\n",
    "            vec = (list(map(float, tokens[1:])))\n",
    "            word = tokens[0].replace(\"'\",'\"')\n",
    "            words.append(word)\n",
    "            word2idx[word]=idx\n",
    "            vectors.append(vec)\n",
    "            idx+=1\n",
    "\n",
    "        vectors = np.array(vectors)\n",
    "        aligned_dict = {w: vectors[word2idx[w]] for w in words}\n",
    "        return aligned_dict\n",
    "    \n",
    "    def get_emb_matrix(pretrained, word_counts, emb_size = 300):\n",
    "        \"\"\" Creates training vocabulary, vocab2index and embedding matrix from pretrained word vectors \"\"\"\n",
    "        found=0\n",
    "        not_found=0\n",
    "        vocab_size = len(word_counts) + 2\n",
    "        vocab_to_idx = {}\n",
    "        vocab = [\"\", \"UNK\"]\n",
    "        W = np.zeros((vocab_size, emb_size), dtype=\"float32\")\n",
    "        W[0] = np.zeros(emb_size, dtype='float32') # adding a vector for padding\n",
    "        W[1] = np.random.uniform(-0.25, 0.25, emb_size) # adding a vector for unknown words \n",
    "        vocab_to_idx[\"UNK\"] = 1\n",
    "        i = 2\n",
    "        for word in word_counts:\n",
    "            if word in pretrained:\n",
    "                W[i] = pretrained[word]\n",
    "                found+=1\n",
    "            else:\n",
    "                W[i] = np.random.uniform(-0.25,0.25, emb_size)\n",
    "                not_found+=1\n",
    "            vocab_to_idx[word] = i\n",
    "            vocab.append(word)\n",
    "            i += 1   \n",
    "        return W, np.array(vocab), vocab_to_idx, found, not_found\n",
    "    \n",
    "    def encode_tokens(tokenized_text, vocab2index, N=50): # keep max doc length to 50 words\n",
    "        \"\"\" Encodes tokenized text into equal length sequences of vocabulary indices \"\"\" \n",
    "        sequences = []\n",
    "        for text in tokenized_text:\n",
    "            seq = np.zeros(N, dtype=int)\n",
    "            # get word index if it's in vocab else get default index\n",
    "            enc = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in text])\n",
    "            # limit sequence length\n",
    "            length = min(N, len(enc))\n",
    "            seq[:length] = enc[:length]\n",
    "            sequences.append(seq)\n",
    "        return sequences\n",
    "\n",
    "    if inference==False:\n",
    "        counts = Counter()\n",
    "        tokenized_text = []\n",
    "        print(\"Tokenizing text and counting unique words\")\n",
    "        for i in dataloader:\n",
    "            tokenized = [tokenize(doc) for doc in i[2]]\n",
    "            counts = count_words(tokenized, counts)\n",
    "            tokenized_text.extend(tokenized)\n",
    "\n",
    "        print(\"Loading pretrained aligned word embeddings\")\n",
    "        aligned_dict = load_embeddings(path)\n",
    "\n",
    "        print(\"Creating vocab2index and embedding matrix with pretrained weights\")\n",
    "        pretrained_weights, vocab, vocab2index, found, not_found = get_emb_matrix(aligned_dict, counts)\n",
    "        sequences = encode_tokens(tokenized_text, vocab2index)\n",
    "        vocab_size = len(vocab)\n",
    "        print(\"Finished preparing sequences\")\n",
    "        return sequences, vocab, vocab_size, vocab2index, pretrained_weights\n",
    "    else:\n",
    "        print(\"Tokenizing text\")\n",
    "        tokenized_text = []\n",
    "        for i in dataloader:\n",
    "            tokenized = [tokenize(doc) for doc in i[2]]\n",
    "            tokenized_text.extend(tokenized)\n",
    "        sequences = encode_tokens(tokenized_text, vocab2index)\n",
    "        print(\"Finished preparing sequences\")\n",
    "        return sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and randomly split dataset into 80:20 training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "valid_size = len(dataset) - train_size\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=False)\n",
    "validation_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing text and counting unique words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1189/158016 [00:00<00:13, 11881.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained aligned word embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 158016/158016 [00:14<00:00, 11070.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocab2index and embedding matrix with pretrained weights\n",
      "Finished preparing sequences\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/kruttikanadig/Documents/Tattle/machine-learning/wiki.hi.align.vec\"\n",
    "train_sequences, vocab, vocab_size, vocab2index, pretrained_weights = prepare_sequences(train_loader, inference=False, path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing text\n",
      "Finished preparing sequences\n"
     ]
    }
   ],
   "source": [
    "valid_sequences = prepare_sequences(validation_loader, inference=True, vocab2index=vocab2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_loader = torch.utils.data.DataLoader(train_sequences, batch_size=8, shuffle=False)\n",
    "valid_seq_loader = torch.utils.data.DataLoader(valid_sequences, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, v in vocab2index.items():\n",
    "#     if v==102:\n",
    "#         print (k)\n",
    "\n",
    "# vocab2index[\"рдХрд░\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, pretrained_weights, num_labels):\n",
    "        super().__init__()\n",
    "        # LSTM layers\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embeddings.weight.data.copy_(torch.from_numpy(pretrained_weights))\n",
    "        self.embeddings.weight.requires_grad = False ## freeze embeddings\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 100)\n",
    "        #self.dropout = nn.Dropout(0.2)\n",
    "        # ResNet layers\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Replace the default ResNet18 classifier layer \n",
    "        classifier_input = self.resnet.fc.in_features\n",
    "        classifier = nn.Sequential(nn.Linear(classifier_input, 100), \n",
    "                             nn.LogSoftmax(dim=1)) \n",
    "        self.resnet.fc = classifier\n",
    "        self.fusion_classifier = nn.Linear(200,2)\n",
    "        \n",
    "    def forward(self, x1, x2): # image vector, encoded image text\n",
    "#         print(\"x1:\",x1)\n",
    "#         print(\"x2\",x2)\n",
    "        # Image\n",
    "        x1_out = self.resnet(x1)\n",
    "        # Text\n",
    "        x2_embed = self.embeddings(x2)\n",
    "        #x2_dropout = self.dropout(x2_embed)\n",
    "#         print(\"img representation:\",x1_out)\n",
    "#         print(\"\")\n",
    "        lstm_out, (ht, ct) = self.lstm(x2_embed)\n",
    "        x2_out = self.linear(ht[-1])\n",
    "#         print(\"text representation:\",x2_out)\n",
    "#         print(\"\")\n",
    "        # Concatenation\n",
    "        fused = torch.cat([x2_out, x1_out], dim=1)\n",
    "#         print(\"fused:\", fused)\n",
    "\n",
    "        return self.fusion_classifier(fused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FusionModel(vocab_size=vocab_size, embedding_dim=300, hidden_dim=5, pretrained_weights=pretrained_weights,\n",
    "                    num_labels=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, epochs, lr, train_loader, train_seq_loader):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    model.train()\n",
    "    for i in range(epochs):\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for i, data in enumerate(zip(train_loader, train_seq_loader)):\n",
    "        #for data in zip(validation_loader, valid_seq_loader): \n",
    "            img_vectors, text_sequences, y_train = data[0][0], data[1], data[0][1]      \n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(img_vectors, text_sequences)\n",
    "            loss = F.cross_entropy(y_pred, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y_train.shape[0]\n",
    "            total += y_train.shape[0]\n",
    "        val_loss, val_acc = validation_metrics(model, validation_loader, valid_seq_loader)\n",
    "        print(\"train loss %.3f, val loss %.3f, val accuracy %.3f\" % (sum_loss/total, val_loss, val_acc))\n",
    "\n",
    "def validation_metrics (model, validation_loader=validation_loader, valid_seq_loader=valid_seq_loader):\n",
    "    # Initialize the prediction and label lists(tensors)\n",
    "    predlist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "    lbllist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    for i, data in enumerate(zip(validation_loader, valid_seq_loader)):\n",
    "        #\n",
    "        img_vectors, text_sequences, y_valid = data[0][0], data[1], data[0][1] \n",
    "        y_hat = model(img_vectors, text_sequences)\n",
    "        loss = F.cross_entropy(y_hat, y_valid)\n",
    "        #print(\"y_hat:\",y_hat)\n",
    "        pred = torch.max(y_hat, 1)[1]\n",
    "        print(\"pred:\",pred)\n",
    "        # Append batch prediction results\n",
    "        predlist=torch.cat([predlist,pred.view(-1).cpu()])\n",
    "        lbllist=torch.cat([lbllist,y_valid.view(-1).cpu()])\n",
    "        # Calculate accuracy\n",
    "        correct += (pred == y_valid).float().sum()\n",
    "        total += y_valid.shape[0]\n",
    "        sum_loss += loss.item()*y_valid.shape[0]\n",
    "    \n",
    "    # Confusion matrix\n",
    "    conf_mat=confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
    "    print(conf_mat)\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    class_accuracy=100*conf_mat.diagonal()/conf_mat.sum(1)\n",
    "    print(class_accuracy)\n",
    "    \n",
    "    return sum_loss/total, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: tensor([0, 1, 1, 0, 0, 1, 0, 1])\n",
      "pred: tensor([0, 1, 0, 1, 1, 1, 0, 1])\n",
      "pred: tensor([1, 0, 1, 1, 1, 1, 1, 1])\n",
      "pred: tensor([0, 1, 1, 0, 0, 1, 0, 0])\n",
      "pred: tensor([0, 0, 1, 0, 0, 1, 1, 1])\n",
      "pred: tensor([1, 1, 1, 1, 0, 1, 0, 1])\n",
      "pred: tensor([0, 1, 1, 1, 1, 1, 0, 1])\n",
      "pred: tensor([1, 0, 0, 1, 0, 1, 1, 1])\n",
      "pred: tensor([0, 0, 1, 0, 0, 0, 1, 1])\n",
      "pred: tensor([1, 1, 0, 1, 1, 1, 1, 0])\n",
      "pred: tensor([0, 0, 1, 1, 1, 0, 0, 1])\n",
      "pred: tensor([0, 1, 0, 0, 1, 1, 0, 0])\n",
      "pred: tensor([1])\n",
      "[[30  7]\n",
      " [10 50]]\n",
      "[81.08108108 83.33333333]\n",
      "train loss 73.679, val loss 694.995, val accuracy 0.825\n",
      "pred: tensor([0, 1, 1, 0, 0, 1, 0, 1])\n",
      "pred: tensor([0, 1, 0, 1, 1, 1, 0, 1])\n",
      "pred: tensor([1, 0, 1, 1, 1, 1, 1, 1])\n",
      "pred: tensor([0, 1, 1, 0, 0, 1, 1, 0])\n",
      "pred: tensor([0, 0, 1, 0, 0, 1, 1, 1])\n",
      "pred: tensor([1, 1, 1, 1, 0, 1, 0, 1])\n",
      "pred: tensor([0, 1, 1, 1, 1, 1, 0, 1])\n",
      "pred: tensor([1, 0, 0, 1, 0, 1, 1, 1])\n",
      "pred: tensor([0, 0, 1, 0, 0, 0, 1, 1])\n",
      "pred: tensor([1, 1, 0, 1, 1, 1, 1, 0])\n",
      "pred: tensor([0, 0, 1, 1, 1, 0, 0, 1])\n",
      "pred: tensor([0, 1, 0, 0, 1, 1, 0, 0])\n",
      "pred: tensor([1])\n",
      "[[29  8]\n",
      " [10 50]]\n",
      "[78.37837838 83.33333333]\n",
      "train loss 8.529, val loss 688.655, val accuracy 0.814\n",
      "pred: tensor([0, 1, 1, 0, 0, 1, 0, 1])\n",
      "pred: tensor([0, 1, 0, 1, 1, 1, 0, 1])\n",
      "pred: tensor([1, 0, 1, 1, 1, 1, 1, 1])\n",
      "pred: tensor([0, 1, 1, 0, 0, 1, 1, 0])\n",
      "pred: tensor([0, 0, 1, 0, 0, 1, 1, 1])\n",
      "pred: tensor([1, 1, 1, 1, 0, 1, 0, 1])\n",
      "pred: tensor([0, 1, 1, 1, 1, 1, 0, 1])\n",
      "pred: tensor([1, 0, 0, 1, 0, 1, 1, 1])\n",
      "pred: tensor([0, 0, 1, 0, 0, 0, 1, 1])\n",
      "pred: tensor([1, 1, 0, 1, 1, 1, 1, 0])\n",
      "pred: tensor([0, 0, 1, 1, 1, 0, 0, 1])\n",
      "pred: tensor([0, 1, 0, 0, 1, 1, 0, 0])\n",
      "pred: tensor([1])\n",
      "[[29  8]\n",
      " [10 50]]\n",
      "[78.37837838 83.33333333]\n",
      "train loss 7.944, val loss 687.549, val accuracy 0.814\n",
      "CPU times: user 2min 57s, sys: 16.7 s, total: 3min 13s\n",
      "Wall time: 2min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_model(model=model, epochs=3, lr=0.001, train_loader=train_loader, train_seq_loader=train_seq_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 1, 0, 0, 1, 0, 1])\n",
      "tensor([0, 1, 0, 1, 1, 1, 0, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([0, 0, 1, 0, 0, 1, 0, 0])\n",
      "tensor([0, 0, 1, 0, 0, 1, 1, 1])\n",
      "tensor([0, 1, 1, 1, 0, 0, 0, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 0, 0])\n",
      "tensor([1, 0, 1, 1, 0, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 0, 0, 1, 1])\n",
      "tensor([1, 1, 1, 0, 1, 1, 1, 0])\n",
      "tensor([1, 1, 1, 1, 1, 0, 0, 1])\n",
      "tensor([0, 1, 0, 1, 0, 1, 0, 0])\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "for i in validation_loader:\n",
    "    print(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.Tensor(([[-4.2521, -4.2464, -4.4741, -4.5547, -4.5478, -5.5457, -5.3049, -5.1204,\n",
    "         -5.1457, -5.2056, -4.5313, -5.0253, -4.7385, -4.7661, -4.5265, -4.2586,\n",
    "         -3.3594, -4.3817, -4.1012, -5.8242, -5.5435, -5.0205, -5.0595, -4.6622,\n",
    "         -4.5196, -4.2702, -4.5995, -4.6876, -5.2176, -3.2149, -4.6365, -4.9898,\n",
    "         -5.5991, -4.2091, -4.7610, -4.1638, -3.8122, -4.7662, -5.0462, -4.9526,\n",
    "         -4.7008, -4.5671, -4.3898, -4.1583, -4.2191, -3.2409, -4.4813, -4.4752,\n",
    "         -2.9030, -4.5563, -4.8537, -4.8357, -4.2699, -4.6099, -5.8208, -5.1968,\n",
    "         -4.3736, -4.7389, -5.1744, -4.9667, -3.9274, -5.3632, -4.8872, -5.0379,\n",
    "         -5.2167, -5.8661, -4.7592, -5.0030, -5.5042, -4.4636, -4.7795, -5.1121,\n",
    "         -4.3879, -4.8093, -5.2627, -5.0186, -5.2898, -4.6608, -4.6598, -5.9072,\n",
    "         -5.5910, -4.7094, -4.7134, -5.2310, -5.1905, -4.3843, -4.2540, -4.8409,\n",
    "         -4.6072, -4.9843, -5.3077, -4.9965, -4.7092, -5.1058, -4.5060, -6.0548,\n",
    "         -5.5167, -5.2616, -5.4040, -5.0175]])\n",
    "               \n",
    "b=torch.Tensor(([[-11.9913,  -3.7826, -12.5833, -11.7889, -11.2992,  -4.7991,  -4.4945,\n",
    "         -12.6855, -11.9649, -12.2276, -11.8422, -11.8447, -12.0405,  -3.7416,\n",
    "         -11.5485, -11.9999, -10.8261,  -3.6566, -12.4659,  -5.4574,  -3.5221,\n",
    "         -13.0376,  -4.1919, -12.3176,  -4.1105, -12.0193, -11.7328, -11.7507,\n",
    "          -4.4628, -11.2013, -12.0474,  -4.1130,  -5.0844, -11.2223,  -3.6835,\n",
    "          -3.1347, -11.4154, -12.1186,  -4.2668,  -4.5831, -11.7406,  -4.6845,\n",
    "          -2.9980,  -3.2289, -12.1989, -10.6418,  -4.1819, -11.4468, -10.9723,\n",
    "         -11.6317,  -4.1813, -12.5192,  -3.4268,  -3.2935,  -5.4684, -12.4832,\n",
    "          -3.8902, -11.5476,  -4.6908,  -4.4863,  -3.9324,  -4.3077, -12.5412,\n",
    "          -3.7257, -12.4882,  -4.7619, -12.2484,  -4.2403, -12.7177, -11.5256,\n",
    "          -4.2174, -12.0858,  -3.4705,  -3.7659, -12.0408,  -3.4726, -12.9526,\n",
    "          -3.7892,  -4.5419, -12.8421, -12.8729,  -3.7096, -11.8118, -12.6065,\n",
    "          -4.6220,  -3.4215,  -3.1535,  -4.5739,  -4.1453,  -4.8709, -12.8969,\n",
    "          -4.0048,  -3.8581,  -4.7366,  -3.8848, -12.9636,  -4.4669,  -5.0619,\n",
    "          -5.0294,  -3.8282],\n",
    "        [-11.8447,  -3.9464, -11.2609, -12.2781, -12.1543,  -3.9545,  -4.0019,\n",
    "         -12.2932, -11.7808, -12.0304, -13.2854, -12.1163, -12.5795,  -3.8014,\n",
    "         -12.4147, -12.2848, -11.4008,  -3.2728, -11.7993,  -4.4218,  -5.2945,\n",
    "         -12.2411,  -3.8866, -12.8508,  -4.0057, -11.6302, -11.6989, -11.8493,\n",
    "          -5.2403,  -9.8713, -11.6305,  -4.2098,  -4.4167, -10.9850,  -3.9780,\n",
    "          -3.3411, -10.9573, -12.2791,  -3.7425,  -4.7395, -11.8754,  -4.2133,\n",
    "          -3.4015,  -3.7881, -12.2334, -10.7329,  -3.8347, -11.5902, -11.0319,\n",
    "         -12.3321,  -3.6539, -12.1957,  -4.0869,  -3.7605,  -5.3315, -12.1294,\n",
    "          -4.1113, -11.4262,  -4.1386,  -4.7603,  -3.1940,  -5.0062, -11.8343,\n",
    "          -4.7427, -13.0524,  -5.2751, -12.5379,  -3.4173, -13.7172, -11.6907,\n",
    "          -3.8422, -12.7018,  -4.2619,  -4.5733, -12.8526,  -4.5814, -12.3846,\n",
    "          -4.2394,  -4.2135, -13.3160, -14.1304,  -4.3743, -11.3125, -12.6819,\n",
    "          -5.7403,  -2.4686,  -4.4718,  -3.4121,  -4.4958,  -3.9187, -12.9499,\n",
    "          -4.2822,  -3.5968,  -4.5703,  -3.0550, -13.7008,  -4.8083,  -3.8884,\n",
    "          -4.3756,  -4.7645]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, data in enumerate(zip(train_loader, train_seq_loader)):\n",
    "#     img_vectors, text_sequences, y_train = data[0][0], data[1], data[0][1]      \n",
    "#     y_pred = model(img_vectors, text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1595852529951,
   "trusted": true
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
