{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import os\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from io import BytesIO\n",
    "import captum\n",
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import NoiseTunnel\n",
    "from torchsummary import summary\n",
    "import wget\n",
    "import sqlite3, json, io\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from pprint import pprint\n",
    "import json\n",
    "import math\n",
    "from math import sqrt\n",
    "import torch\n",
    "import itertools \n",
    "import torchtext\n",
    "from torchtext.data import Field, Dataset, Example\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from google.cloud import vision\n",
    "import sys,os,json\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from google.cloud import vision\n",
    "from google.protobuf.json_format import MessageToJson\n",
    "import requests\n",
    "import logging\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "from torch.utils.data import DataLoader\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import io\n",
    "from io import BytesIO\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wget.download(\"https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.hi.align.vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images and image text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations=transforms.Compose([\n",
    "                    transforms.Resize((224,224)), \n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, txt_path, img_dir, \n",
    "                 transform):\n",
    "        df = pd.read_csv(txt_path, sep=\" \", header=None, names= [\"img_name\"])\n",
    "        df[\"label\"] = df[\"img_name\"].apply(lambda x: 0 if \"neg\" in x else 1)\n",
    "        self.img_dir=img_dir\n",
    "        self.txt_path=txt_path\n",
    "        self.img_names=df[\"img_name\"].values\n",
    "        self.y = df[\"label\"].values\n",
    "        self.transform=transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(os.path.join(self.img_dir, self.img_names[idx])).convert('RGB')\n",
    "        client = vision.ImageAnnotatorClient()\n",
    "        img_text = self.get_image_text(os.path.join(self.img_dir, self.img_names[idx]), client)\n",
    "        if self.transform is not None:\n",
    "            img_vector=self.transform(img)\n",
    "        \n",
    "        label = self.y[idx]\n",
    "        return img_vector,label,img_text\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "    \n",
    "    def get_image_text(self, path, client):  \n",
    "        def detect_text(img_bytes, client):\n",
    "            image_data = vision.types.Image(content=img_bytes)\n",
    "            resp = client.text_detection(image=image_data)\n",
    "            resp = json.loads(MessageToJson(resp))\n",
    "            text = resp.get('fullTextAnnotation',{}).get(\"text\",\"\")\n",
    "            return text\n",
    "        \n",
    "        image = Image.open(path, mode=\"r\")\n",
    "        imgByteArr = io.BytesIO()\n",
    "        image.save(imgByteArr, format=\"PNG\")\n",
    "        img_bytes = imgByteArr.getvalue()\n",
    "        text = detect_text(img_bytes, client)\n",
    "        return text\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = CustomDataset(txt_path=\"/Users/kruttikanadig/Documents/Tattle/machine-learning/textfile.txt\",\n",
    "#                        img_dir=\"/Users/kruttikanadig/Documents/Tattle/machine-learning/all\",\n",
    "#                        transform=transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(txt_path=\"/Users/kruttikanadig/Documents/Tattle/machine-learning/temp.txt\",\n",
    "                       img_dir=\"/Users/kruttikanadig/Documents/Tattle/machine-learning/few\",\n",
    "                       transform=transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create text preprocessing pipeline\n",
    "\n",
    "1. Tokenize training data and get word counts\n",
    "2. Load aligned word embeddings into aligned dict \n",
    "2. Create vocab and vocab2index using aligned dict and word counts\n",
    "3. Encode training data using vocab and vocab2index \n",
    "4. Apply to validation / test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(dataloader, inference=False, path=None, vocab2index=None):\n",
    "    \"\"\" Encodes text into sequences of vocabulary indices \"\"\"\n",
    "    \n",
    "    def tokenize(text):\n",
    "        \"\"\" Removes punctuation and numbers, converts to lowercase and splits into individual words \"\"\"\n",
    "        regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n\\।]') # last character । is the Hindi full stop\n",
    "        nopunct = regex.sub(\" \", text.lower())\n",
    "        tokenized_text = [token for token in nopunct.split(\" \") if len(token) > 0]\n",
    "        return tokenized_text\n",
    "    \n",
    "    def count_words(tokens, counts):\n",
    "        \"\"\" Counts unique words in text \"\"\"\n",
    "        counts.update(list(chain.from_iterable(tokens)))\n",
    "        return counts\n",
    "    \n",
    "    def load_embeddings(path):\n",
    "        \"\"\" Loads pretrained aligned word embeddings \"\"\"\n",
    "        words = []\n",
    "        idx = 0\n",
    "        word2idx = {}\n",
    "        vectors = []\n",
    "        fin = io.open(path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "        n, d = map(int, fin.readline().split())\n",
    "        for line in tqdm.tqdm(fin,total=n):\n",
    "            tokens = line.rstrip().split(' ')\n",
    "            vec = (list(map(float, tokens[1:])))\n",
    "            word = tokens[0].replace(\"'\",'\"')\n",
    "            words.append(word)\n",
    "            word2idx[word]=idx\n",
    "            vectors.append(vec)\n",
    "            idx+=1\n",
    "\n",
    "        vectors = np.array(vectors)\n",
    "        aligned_dict = {w: vectors[word2idx[w]] for w in words}\n",
    "        return aligned_dict\n",
    "    \n",
    "    def get_emb_matrix(pretrained, word_counts, emb_size = 300):\n",
    "        \"\"\" Creates training vocabulary, vocab2index and embedding matrix from pretrained word vectors \"\"\"\n",
    "        found=0\n",
    "        not_found=0\n",
    "        vocab_size = len(word_counts) + 2\n",
    "        vocab_to_idx = {}\n",
    "        vocab = [\"\", \"UNK\"]\n",
    "        W = np.zeros((vocab_size, emb_size), dtype=\"float32\")\n",
    "        W[0] = np.zeros(emb_size, dtype='float32') # adding a vector for padding\n",
    "        W[1] = np.random.uniform(-0.25, 0.25, emb_size) # adding a vector for unknown words \n",
    "        vocab_to_idx[\"UNK\"] = 1\n",
    "        i = 2\n",
    "        for word in word_counts:\n",
    "            if word in pretrained:\n",
    "                W[i] = pretrained[word]\n",
    "                found+=1\n",
    "            else:\n",
    "                W[i] = np.random.uniform(-0.25,0.25, emb_size)\n",
    "                not_found+=1\n",
    "            vocab_to_idx[word] = i\n",
    "            vocab.append(word)\n",
    "            i += 1   \n",
    "        return W, np.array(vocab), vocab_to_idx, found, not_found\n",
    "    \n",
    "    def encode_tokens(tokenized_text, vocab2index, N=50): # keep max doc length to 50 words\n",
    "        \"\"\" Encodes tokenized text into equal length sequences of vocabulary indices \"\"\" \n",
    "        sequences = []\n",
    "        for text in tokenized_text:\n",
    "            seq = np.zeros(N, dtype=int)\n",
    "            # get word index if it's in vocab else get default index\n",
    "            enc = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in text])\n",
    "            # limit sequence length\n",
    "            length = min(N, len(enc))\n",
    "            seq[:length] = enc[:length]\n",
    "            sequences.append(seq)\n",
    "        return sequences\n",
    "\n",
    "    if inference==False:\n",
    "        counts = Counter()\n",
    "        tokenized_text = []\n",
    "        print(\"Tokenizing text and counting unique words\")\n",
    "        for i in dataloader:\n",
    "            tokenized = [tokenize(doc) for doc in i[2]]\n",
    "            counts = count_words(tokenized, counts)\n",
    "            tokenized_text.extend(tokenized)\n",
    "\n",
    "        print(\"Loading pretrained aligned word embeddings\")\n",
    "        aligned_dict = load_embeddings(path)\n",
    "\n",
    "        print(\"Creating vocab2index and embedding matrix with pretrained weights\")\n",
    "        pretrained_weights, vocab, vocab2index, found, not_found = get_emb_matrix(aligned_dict, counts)\n",
    "        sequences = encode_tokens(tokenized_text, vocab2index)\n",
    "        vocab_size = len(vocab)\n",
    "        print(\"Finished preparing sequences\")\n",
    "        return sequences, vocab, vocab_size, vocab2index, pretrained_weights\n",
    "    else:\n",
    "        print(\"Tokenizing text\")\n",
    "        tokenized_text = []\n",
    "        for i in dataloader:\n",
    "            tokenized = [tokenize(doc) for doc in i[2]]\n",
    "            tokenized_text.extend(tokenized)\n",
    "        sequences = encode_tokens(tokenized_text, vocab2index)\n",
    "        print(\"Finished preparing sequences\")\n",
    "        return sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and randomly split dataset into 80:20 training and validation sets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "valid_size = len(dataset) - train_size\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=False)\n",
    "validation_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-2.0323, -2.0323, -2.0323,  ..., -2.0152, -2.0152, -2.0323],\n",
      "         [-2.0323, -2.0323, -2.0323,  ..., -1.9467, -1.9638, -2.0323],\n",
      "         [-2.0323, -2.0323, -2.0323,  ..., -1.9124, -1.9124, -1.9980],\n",
      "         ...,\n",
      "         [-1.5870, -1.5870, -1.5870,  ..., -1.3644, -1.3473, -1.3473],\n",
      "         [-1.5699, -1.5699, -1.5699,  ..., -1.3644, -1.3644, -1.3815],\n",
      "         [-1.5357, -1.5357, -1.5357,  ..., -1.3644, -1.3815, -1.4158]],\n",
      "\n",
      "        [[-0.6877, -0.6877, -0.6877,  ..., -0.7052, -0.6877, -0.6702],\n",
      "         [-0.6877, -0.6877, -0.6877,  ..., -0.6702, -0.6702, -0.6702],\n",
      "         [-0.6877, -0.6877, -0.6877,  ..., -0.7227, -0.6527, -0.6352],\n",
      "         ...,\n",
      "         [-0.7402, -0.7402, -0.7402,  ..., -0.3550, -0.3375, -0.3375],\n",
      "         [-0.7227, -0.7227, -0.7227,  ..., -0.3550, -0.3550, -0.3725],\n",
      "         [-0.6877, -0.6877, -0.6877,  ..., -0.3550, -0.3725, -0.4076]],\n",
      "\n",
      "        [[ 0.8274,  0.8274,  0.8274,  ...,  0.7054,  0.7228,  0.7576],\n",
      "         [ 0.8274,  0.8274,  0.8274,  ...,  0.6705,  0.6879,  0.7576],\n",
      "         [ 0.8274,  0.8274,  0.8274,  ...,  0.5659,  0.6531,  0.7925],\n",
      "         ...,\n",
      "         [ 0.2696,  0.2696,  0.2696,  ...,  0.6879,  0.7054,  0.7054],\n",
      "         [ 0.2871,  0.2871,  0.2871,  ...,  0.6879,  0.6879,  0.6705],\n",
      "         [ 0.3219,  0.3219,  0.3219,  ...,  0.6879,  0.6705,  0.6356]]]), 0, '')\n",
      "(tensor([[[-0.8164, -0.8164, -0.7137,  ...,  2.2318,  2.2318,  2.2318],\n",
      "         [-0.7822, -0.7479, -0.6452,  ...,  2.2318,  2.2318,  2.2318],\n",
      "         [-0.5082, -0.4568, -0.4911,  ...,  2.2147,  2.2318,  2.2147],\n",
      "         ...,\n",
      "         [ 0.1254,  0.3652,  0.4166,  ...,  0.1083, -0.0801, -0.1999],\n",
      "         [ 0.1939,  0.3652,  0.3994,  ...,  0.0227, -0.1486, -0.2171],\n",
      "         [ 0.2967,  0.3309,  0.2796,  ..., -0.0458, -0.2342, -0.2856]],\n",
      "\n",
      "        [[-0.0924, -0.1450, -0.1275,  ...,  2.4286,  2.4286,  2.4286],\n",
      "         [-0.1275, -0.1275, -0.0574,  ...,  2.4286,  2.4286,  2.4286],\n",
      "         [ 0.0301,  0.1001,  0.1001,  ...,  2.4111,  2.4111,  2.4111],\n",
      "         ...,\n",
      "         [ 0.4503,  0.6779,  0.6954,  ...,  0.8880,  0.7829,  0.7304],\n",
      "         [ 0.2402,  0.4153,  0.4678,  ...,  0.8354,  0.7304,  0.7304],\n",
      "         [ 0.1176,  0.2052,  0.2402,  ...,  0.7654,  0.6429,  0.6429]],\n",
      "\n",
      "        [[-1.4036, -1.4210, -1.3687,  ...,  2.5529,  2.5529,  2.5529],\n",
      "         [-1.4559, -1.4384, -1.3513,  ...,  2.5529,  2.5529,  2.5529],\n",
      "         [-1.3339, -1.2641, -1.2641,  ...,  2.5354,  2.5529,  2.5354],\n",
      "         ...,\n",
      "         [-0.1487,  0.0953,  0.1825,  ...,  0.0256,  0.0431,  0.0431],\n",
      "         [-0.2707, -0.0615,  0.0256,  ..., -0.1835, -0.0964, -0.0092],\n",
      "         [-0.3230, -0.2184, -0.1661,  ..., -0.3578, -0.2358, -0.1138]]]), 0, 'OA Shot on OnePlus\\nBy Ankesh\\n')\n",
      "(tensor([[[-0.6965, -0.6965, -0.6965,  ..., -1.3644, -1.4158, -1.3644],\n",
      "         [-0.6965, -0.6965, -0.6965,  ..., -1.2274, -1.3815, -1.4329],\n",
      "         [-0.6965, -0.6965, -0.6965,  ..., -1.0904, -1.3130, -1.4843],\n",
      "         ...,\n",
      "         [ 1.3927,  1.3927,  1.3927,  ...,  1.3927,  1.3927,  1.3927],\n",
      "         [ 1.3755,  1.3755,  1.3755,  ...,  1.3755,  1.3755,  1.3755],\n",
      "         [ 1.4098,  1.4098,  1.4098,  ...,  1.4098,  1.4098,  1.4098]],\n",
      "\n",
      "        [[ 0.9930,  0.9930,  0.9930,  ..., -0.3901, -0.4076, -0.3375],\n",
      "         [ 0.9930,  0.9930,  0.9930,  ..., -0.2500, -0.3725, -0.4076],\n",
      "         [ 0.9930,  0.9930,  0.9930,  ..., -0.0924, -0.3025, -0.4601],\n",
      "         ...,\n",
      "         [ 1.5532,  1.5532,  1.5532,  ...,  1.5532,  1.5532,  1.5532],\n",
      "         [ 1.5357,  1.5357,  1.5357,  ...,  1.5357,  1.5357,  1.5357],\n",
      "         [ 1.5707,  1.5707,  1.5707,  ...,  1.5707,  1.5707,  1.5707]],\n",
      "\n",
      "        [[ 2.3088,  2.3088,  2.3088,  ...,  1.0017,  0.8099,  0.7925],\n",
      "         [ 2.3088,  2.3088,  2.3088,  ...,  1.1411,  0.8448,  0.7054],\n",
      "         [ 2.3088,  2.3088,  2.3088,  ...,  1.2980,  0.9319,  0.6705],\n",
      "         ...,\n",
      "         [ 1.7685,  1.7685,  1.7685,  ...,  1.7685,  1.7685,  1.7685],\n",
      "         [ 1.7511,  1.7511,  1.7511,  ...,  1.7511,  1.7511,  1.7511],\n",
      "         [ 1.7860,  1.7860,  1.7860,  ...,  1.7860,  1.7860,  1.7860]]]), 0, 'पटमात्मा का\\nनाम करबीर है।\\nपवित्र अथर्ववेद काण्ड नं. 4 अनुवाक नं. 1 मंत्र 7\\nयोथर्वाणं पित्तरं देवबन्धुं बहस्पतिं नमसाव च गच्छात् । त्वं विश्वेषां जनिता\\nयथासः कविदेवो न दभायत् स्वधावान्।।\\nपरमेश्वर का नाम कविर्देव है जो अविनाशी जगत् गुरु, आत्माधार,\\nपूर्ण मुक्त होकर सत्यलोक गए हैं उनको सतलोक ले जाने वाला,\\nब्रह्मण्डों का रचनहार, काल की तरह धोखा न देने वाला कबीर प्रभु है।\\nजो\\nसर्व\\nअधिक जानकारी के लिए पवित्र पुस्तक \"ज्ञान गंगा\" नि:शुल्क प्राप्त करें। अपना नाम,\\nपूरा पता, मोबाइल नंबर हमें Whatsapp करें +91 7496801825\\nSPIRITUAL LEADER\\nSANT RAMPAL JI\\nfO in\\n@SAINTRAMPALJIM\\nSUPREMEGOD.ORG\\nSAINT RAMPAL JI\\nMAHARAJ\\n')\n"
     ]
    }
   ],
   "source": [
    "len(dataset)\n",
    "for i in dataset:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing text and counting unique words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1979/158016 [00:00<00:15, 9922.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained aligned word embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 158016/158016 [00:19<00:00, 8131.42it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocab2index and embedding matrix with pretrained weights\n",
      "Finished preparing sequences\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/kruttikanadig/Documents/Tattle/machine-learning/wiki.hi.align.vec\"\n",
    "train_sequences, vocab, vocab_size, vocab2index, pretrained_weights = prepare_sequences(train_loader, inference=False, path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing text\n",
      "Finished preparing sequences\n"
     ]
    }
   ],
   "source": [
    "valid_sequences = prepare_sequences(validation_loader, inference=True, vocab2index=vocab2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_loader = torch.utils.data.DataLoader(train_sequences, batch_size=2, shuffle=False)\n",
    "valid_seq_loader = torch.utils.data.DataLoader(valid_sequences, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([tensor([[[[ 2.2489,  2.2489,  2.2489,  ...,  2.2147,  2.2489,  2.2489],\n",
      "          [ 2.2489,  2.2489,  2.2489,  ...,  2.2147,  2.2489,  2.2489],\n",
      "          [ 2.2489,  2.2489,  2.2489,  ...,  2.2147,  2.2489,  2.2489],\n",
      "          ...,\n",
      "          [ 1.3927,  1.3413,  1.3413,  ...,  1.3755,  1.3755,  1.3755],\n",
      "          [ 1.3584,  1.3584,  1.3584,  ...,  1.3584,  1.3584,  1.3584],\n",
      "          [ 1.3584,  1.3584,  1.3584,  ...,  1.3584,  1.3584,  1.3584]],\n",
      "\n",
      "         [[ 2.0959,  2.0959,  2.0959,  ...,  2.0609,  2.0959,  2.0959],\n",
      "          [ 2.0959,  2.0959,  2.0959,  ...,  2.0609,  2.0959,  2.0959],\n",
      "          [ 2.0959,  2.0959,  2.0959,  ...,  2.0609,  2.0959,  2.0959],\n",
      "          ...,\n",
      "          [ 1.5532,  1.5007,  1.5007,  ...,  1.5357,  1.5357,  1.5357],\n",
      "          [ 1.5182,  1.5182,  1.5182,  ...,  1.5182,  1.5182,  1.5182],\n",
      "          [ 1.5182,  1.5182,  1.5182,  ...,  1.5182,  1.5182,  1.5182]],\n",
      "\n",
      "         [[-0.7936, -0.7936, -0.7936,  ..., -0.8284, -0.7936, -0.7936],\n",
      "          [-0.7936, -0.7936, -0.7936,  ..., -0.8284, -0.7936, -0.7936],\n",
      "          [-0.7936, -0.7936, -0.7936,  ..., -0.8284, -0.7936, -0.7936],\n",
      "          ...,\n",
      "          [ 1.7685,  1.7163,  1.7163,  ...,  1.7511,  1.7511,  1.7511],\n",
      "          [ 1.7337,  1.7337,  1.7337,  ...,  1.7337,  1.7337,  1.7337],\n",
      "          [ 1.7337,  1.7337,  1.7337,  ...,  1.7337,  1.7337,  1.7337]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4612,  1.4612,  1.4612,  ...,  1.4612,  1.4612,  1.4612],\n",
      "          [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],\n",
      "          [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],\n",
      "          ...,\n",
      "          [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],\n",
      "          [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],\n",
      "          [ 1.4612,  1.4612,  1.4612,  ...,  1.4612,  1.4612,  1.4612]],\n",
      "\n",
      "         [[ 1.6232,  1.6232,  1.6232,  ...,  1.6232,  1.6232,  1.6232],\n",
      "          [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],\n",
      "          [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],\n",
      "          ...,\n",
      "          [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],\n",
      "          [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],\n",
      "          [ 1.6232,  1.6232,  1.6232,  ...,  1.6232,  1.6232,  1.6232]],\n",
      "\n",
      "         [[ 1.8383,  1.8383,  1.8383,  ...,  1.8383,  1.8383,  1.8383],\n",
      "          [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],\n",
      "          [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],\n",
      "          ...,\n",
      "          [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],\n",
      "          [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],\n",
      "          [ 1.8383,  1.8383,  1.8383,  ...,  1.8383,  1.8383,  1.8383]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5022,  0.5022,  0.5022,  ...,  0.5022,  0.5022,  0.5022],\n",
      "          [ 0.5022,  0.5022,  0.5022,  ...,  0.5022,  0.5022,  0.5022],\n",
      "          [ 0.5022,  0.5022,  0.5022,  ...,  0.5022,  0.5022,  0.5022],\n",
      "          ...,\n",
      "          [ 0.5022,  0.5022,  0.5022,  ...,  0.5022,  0.5022,  0.5022],\n",
      "          [ 0.5022,  0.5022,  0.5022,  ...,  0.5022,  0.5022,  0.5022],\n",
      "          [ 0.5022,  0.5022,  0.5022,  ...,  0.5022,  0.5022,  0.5022]],\n",
      "\n",
      "         [[ 0.0651,  0.0651,  0.0651,  ...,  0.0651,  0.0651,  0.0651],\n",
      "          [ 0.0651,  0.0651,  0.0651,  ...,  0.0651,  0.0651,  0.0651],\n",
      "          [ 0.0651,  0.0651,  0.0651,  ...,  0.0651,  0.0651,  0.0651],\n",
      "          ...,\n",
      "          [ 0.0651,  0.0651,  0.0651,  ...,  0.0651,  0.0651,  0.0651],\n",
      "          [ 0.0651,  0.0651,  0.0651,  ...,  0.0651,  0.0651,  0.0651],\n",
      "          [ 0.0651,  0.0651,  0.0651,  ...,  0.0651,  0.0651,  0.0651]],\n",
      "\n",
      "         [[-1.1247, -1.1247, -1.1247,  ..., -1.1247, -1.1247, -1.1247],\n",
      "          [-1.1247, -1.1247, -1.1247,  ..., -1.1247, -1.1247, -1.1247],\n",
      "          [-1.1247, -1.1247, -1.1247,  ..., -1.1247, -1.1247, -1.1247],\n",
      "          ...,\n",
      "          [-1.1247, -1.1247, -1.1247,  ..., -1.1247, -1.1247, -1.1247],\n",
      "          [-1.1247, -1.1247, -1.1247,  ..., -1.1247, -1.1247, -1.1247],\n",
      "          [-1.1247, -1.1247, -1.1247,  ..., -1.1247, -1.1247, -1.1247]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.5082, -0.7479, -0.9192,  ..., -0.7308, -0.7137, -1.0048],\n",
      "          [-0.5596, -0.7137, -0.7650,  ..., -0.5767, -0.5767, -0.9192],\n",
      "          [-0.5424, -0.5938, -0.5767,  ..., -0.4911, -0.4911, -0.8335],\n",
      "          ...,\n",
      "          [-0.4739, -0.4739, -0.4911,  ..., -1.6384, -1.6042, -1.6555],\n",
      "          [-0.4911, -0.4911, -0.4911,  ..., -1.6898, -1.6384, -1.7240],\n",
      "          [-0.4911, -0.4911, -0.4911,  ..., -1.6727, -1.6555, -1.7412]],\n",
      "\n",
      "         [[-0.2850, -0.5301, -0.7052,  ..., -0.6527, -0.6352, -0.9328],\n",
      "          [-0.3375, -0.4951, -0.5476,  ..., -0.5651, -0.5651, -0.9153],\n",
      "          [-0.3200, -0.3725, -0.3550,  ..., -0.5826, -0.5826, -0.9153],\n",
      "          ...,\n",
      "          [-0.4251, -0.4251, -0.4426,  ..., -1.7556, -1.7206, -1.7731],\n",
      "          [-0.4426, -0.4426, -0.4426,  ..., -1.8081, -1.7556, -1.8256],\n",
      "          [-0.4426, -0.4426, -0.4426,  ..., -1.7906, -1.7731, -1.8431]],\n",
      "\n",
      "         [[ 0.2173, -0.0267, -0.2010,  ..., -0.4798, -0.4624, -0.7587],\n",
      "          [ 0.1651,  0.0082, -0.0441,  ..., -0.3753, -0.3753, -0.7238],\n",
      "          [ 0.1825,  0.1302,  0.1476,  ..., -0.3578, -0.3578, -0.6890],\n",
      "          ...,\n",
      "          [-0.2532, -0.2532, -0.2707,  ..., -1.5256, -1.4907, -1.5430],\n",
      "          [-0.2707, -0.2707, -0.2707,  ..., -1.5779, -1.5256, -1.5953],\n",
      "          [-0.2707, -0.2707, -0.2707,  ..., -1.5604, -1.5430, -1.6127]]],\n",
      "\n",
      "\n",
      "        [[[-1.1760, -1.1760, -1.1760,  ..., -0.8335, -0.8335, -0.8335],\n",
      "          [-1.1760, -1.1760, -1.1760,  ..., -0.8335, -0.8335, -0.8335],\n",
      "          [-1.1760, -1.1760, -1.1760,  ..., -0.8335, -0.8335, -0.8335],\n",
      "          ...,\n",
      "          [-1.5014, -1.5014, -1.5014,  ..., -1.1760, -1.1760, -1.1418],\n",
      "          [-1.5185, -1.5528, -1.5528,  ..., -1.2103, -1.1760, -1.1418],\n",
      "          [-1.5185, -1.5870, -1.5699,  ..., -1.2103, -1.1760, -1.1760]],\n",
      "\n",
      "         [[-0.7052, -0.7052, -0.7052,  ..., -0.5301, -0.5301, -0.5301],\n",
      "          [-0.7052, -0.7052, -0.7052,  ..., -0.5301, -0.5301, -0.5301],\n",
      "          [-0.7052, -0.7052, -0.7052,  ..., -0.5301, -0.5301, -0.5301],\n",
      "          ...,\n",
      "          [-1.4055, -1.4055, -1.4055,  ..., -1.2479, -1.2479, -1.2129],\n",
      "          [-1.4230, -1.4580, -1.4580,  ..., -1.2829, -1.2479, -1.2129],\n",
      "          [-1.4230, -1.4930, -1.4755,  ..., -1.2829, -1.2479, -1.2479]],\n",
      "\n",
      "         [[ 0.0256,  0.0256,  0.0256,  ...,  0.1825,  0.1825,  0.1825],\n",
      "          [ 0.0256,  0.0256,  0.0256,  ...,  0.1825,  0.1825,  0.1825],\n",
      "          [ 0.0256,  0.0256,  0.0256,  ...,  0.1825,  0.1825,  0.1825],\n",
      "          ...,\n",
      "          [-1.1770, -1.1770, -1.1770,  ..., -1.0550, -1.0550, -1.0201],\n",
      "          [-1.1944, -1.2293, -1.2293,  ..., -1.0898, -1.0550, -1.0201],\n",
      "          [-1.1944, -1.2641, -1.2467,  ..., -1.0898, -1.0550, -1.0550]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0948,  2.1119,  2.0948,  ...,  2.0948,  2.0948,  2.0948],\n",
      "          [ 2.0948,  2.0948,  1.9920,  ...,  2.0605,  2.0948,  2.0948],\n",
      "          [ 2.0948,  2.0777,  1.1700,  ...,  1.9749,  2.0777,  2.0948],\n",
      "          ...,\n",
      "          [ 2.1290,  2.0948,  2.0263,  ...,  1.9920,  2.0948,  2.0948],\n",
      "          [ 2.0948,  2.0948,  2.0777,  ...,  2.0777,  2.0948,  2.0948],\n",
      "          [ 2.0777,  2.0948,  2.0948,  ...,  2.1119,  2.0948,  2.0948]],\n",
      "\n",
      "         [[ 2.2710,  2.2885,  2.2710,  ...,  2.2710,  2.2710,  2.2710],\n",
      "          [ 2.2710,  2.2710,  2.1660,  ...,  2.2360,  2.2710,  2.2710],\n",
      "          [ 2.2710,  2.2535,  1.3256,  ...,  2.1485,  2.2535,  2.2710],\n",
      "          ...,\n",
      "          [ 2.2710,  2.2535,  2.1835,  ...,  2.1660,  2.2710,  2.2710],\n",
      "          [ 2.2885,  2.2710,  2.2535,  ...,  2.2535,  2.2710,  2.2710],\n",
      "          [ 2.2885,  2.2710,  2.2710,  ...,  2.2885,  2.2710,  2.2710]],\n",
      "\n",
      "         [[ 2.4831,  2.5006,  2.4831,  ...,  2.4831,  2.4831,  2.4831],\n",
      "          [ 2.4831,  2.4831,  2.3786,  ...,  2.4483,  2.4831,  2.4831],\n",
      "          [ 2.4831,  2.4657,  1.5420,  ...,  2.3611,  2.4657,  2.4831],\n",
      "          ...,\n",
      "          [ 2.4831,  2.4483,  2.3960,  ...,  2.3786,  2.4831,  2.4831],\n",
      "          [ 2.3786,  2.3960,  2.3960,  ...,  2.4657,  2.4831,  2.4831],\n",
      "          [ 2.4483,  2.4483,  2.4657,  ...,  2.5006,  2.4831,  2.4831]]]]), tensor([0, 1, 0, 1, 1, 1, 0, 0]), ('KABIR SAHEB A ALLAH HAIN\\nआयत 52 :- फला तुतिअल् - काफिरन् व\\nजहिढहुम बिही जिहाढन् कबीराकबीर्)\\nहजरत मुहम्मद जी को खुदा (प्रभु) कह रहे है कि हे पैगम्बर ! आप काफिरों\\n(जो एक प्रभु की भक्ति त्याग कर अन्य देवी-देवताओं तथा मूर्ति आदि\\nकी पूजा करते हैं) का कहा मत मानना, क्योंकि वे लोग कबीर को पूर्ण\\nपरमात्मा नहीं मानते। आप मेरे द्वारा दिए इस कुरान के ज्ञान के आधार\\nपर अटल रहना कि कबीर ही पूर्ण प्रभु है तथा कबीर अल्लाह के लिए\\nसंघर्ष करना (लड़ना नहीं) अर्थात् अडिग रहना।\\nFREE\\nGyan Gariga\\nअधिक जानकारी के लिए\\nपवित्र पुस्तक\"ज्ञान गंगा नि:शुल्क प्राप्त करें।\\nअपना नाम, पूरा पता, मोबाइल नंबर हमें Whatsapp करें -9496801825\\nE SUPREMEGOD.ORG\\nSANT RAMPAL JI\\nSPIRITUAL LEADER\\n@SAINTRAMPALJIM\\nin P\\nMAHARAJ\\nSAINT RAMPAL JI\\nFREE\\n', '4G. 4G\\nl 12:32\\nO Y, 40 92%\\n0.00\\nLTE1\\n+ Varun Jain\\nआवश्यक सूचना:-\\nअभी अभी जानकारी मिली है कि ग्राम नागेलाव वाया पीसांगन जिला अजमेर\\nमें एक बालिका का जन्म हॉस्पिटल में हुआ। बालिका ने जन्म लेते ही बोली\\nकि भारत में जो कोरोना वायरस संक्रमण फैला हुआ है उसके बचाव के लिए\\nभारत के प्रत्येक नागरिक को अपने दाएं पैर के अंगूठे के नाखून पर हल्दी का\\nलेप (मेहंदी की तरह) लगाना है। इससे कोरोना का संक्रमण समाप्त हो\\nजाएगा सभी नागरिक सकुशल रहेंगे। यह कहकर बालिका की उसी समय\\nमृत्यु हो गई यह देखकर अस्पताल के डॉक्टर भी आश्चर्यचकित हो गए। अतः\\nआपसे निवेदन है कि आप भी तत्काल इस तरह का लेप अपने दाएं पैर के\\nअंगूठे के नाखून पर लगाकर कोरोना वायरस संक्रमण से अपना एवं अपने\\nपरिवार का जीवन को बचाएं। यह फेक न्यूज नहीं है सत्य घटना है।\\nपुरुष शौचालय\\nओ. पी. डी.\\nGENTS TOILET\\nO.P.D.\\nIMMUNIZATION\\nELECTRICAL ROOM\\nSTAIRGASE\\nD.D. W. STORE\\nटीकाकरण कक्ष\\nविद्युत - कक्ष\\nD১|दवा भण्डार\\nSTORE\\n-2 मुर्दा चीर-काड कक्ष POST MAP\\n| स्वास्थ्य केन्द्र-कालन\\nSince 19\\nLike\\nComment\\nShare\\nWrite a comment.\\n(GIF\\n', 'कोरोना बचाने के\\nपुलिस वालीं के मंत्र\\nघर से बाहर न निकले\\nनिकले तो तोड़ देंगे\\nशरीर का कोना कोना\\nमगर होने नही द\\nकिसी को कोरोना ।।\\n', 'Corona महामारी को देखते हुए\\nTiger Shroff ने सरकार को दीया\\n350 करोड़ का चेक\\n[CDS)\\nरीयल हीरो ह\\nTiger Shroff\\n', 'परिषद\\nCANTONMERT BOARO\\nसत्यमेद उयते\\nकोरोना वायरस एलर्ट\\nजन स्वास्थ्य हित में जारी\\nबचाव एवं उपचार -\\n1. अपने घर में कहीं भी आर्द्र वातावरण न होने दें।\\n2. कपड़ों को तेज धूप में सुखायें।\\n3. नीबू के पतले टुकड़े गुनगुने पानी के साथ दिनभर पीते रहें। नीदू में उपस्थित विटामिन\\nसी आपकी प्रतिरक्षा शक्ति बढ़ाता है। विटामिन सी घुलनशील होने की वजह से\\nदिनभरइसका सेवन करना लाभदायक होगा।\\n4. कच्चे सलाद (जैसे-मूली, गाजर, टमाटर इत्यादि) न खायें ।\\n5. जो भी खायें भरपूर पका कर खायें। फल सिर्फ वहीं खयें जिसके पूरे छिलके उतर जाते\\nहैं। जैसे-केला, सन्तरा।\\n6. अपने हाथों को साबुन/एल्कोहलयुक्त सेनेटाइजर से लगातार धोते रहें। हाथ से चेहरे\\nकोनछुवें। जब भी खाँसे या छिक्के पेपर नैपकिन का प्रयोग अवश्य करें।\\n7. कार्यस्थल पर कार्य करने के दौरान जिन जगहों पर आपका हाथ ज्यादा जाता है, उन\\nस्थानों को एल्कोहल स्वॉब(Alchohol Swab) से धोने का प्रयत्न करें।\\n৪. अपने मोबाइल फोनकी स्क्रीन को यथासम्भव साफ रखें।\\n9. मोबाइल फोन को कान या मुँह के करीब कम-से-कम रखें।\\n10. पेट भरारखें। भूखे नरहें।\\nइस तरह के वायरस /वेक्टीरिया भूखे रहने वाले व्यक्तियों पर ज्यादा तेजी से अपना\\nदुष्प्रभाव डालते हैं।\\n11. कोरोना वायरस अधिक तापमान में नहीं टिकता है। तापमान जैसे ही 30 - 35 डिग्री\\nपहुँचेगा कोरोना वायरस खुद-बखुद निष्प्रभावी हो जायेगा।\\n12. किसी भी अफवाह का हिस्सा न बनें, जागरूकता ही आपका बचाव है।\\n', 'जरूरी सूचना\\nपुलिस की गाड़ी आती देखकर घर\\nमें भाग जाना और फिर गाड़ी जाते\\nही बाहर आ जाना यह धोका आप\\nपुलिस को नहीं अपने परिवार और\\nस्वयं को दे रहे हो।॥\\nPlease Stay Home And Be Safe\\n', '', 'MelkaniQuotes\\nझूठी बात पर जो वाह\\nकरेंगे\\nवही लोग आपको तबाह\\nकरेंगे\\nMelkani\\nQuotes\\n')], tensor([[ 873,    1,  144,    1,    1,    1,    1,    1,    1,  402,    1,    1,\n",
      "            1,    1,    1,    1,  408,   57,    1, 4020, 1100,   68,   11,   81,\n",
      "         4057,    1,  109,    1,   23,  207, 4020,   19, 5031, 1151,  244, 2692,\n",
      "         3076, 1507,  412,    1,  857,   19, 3608, 1486,   17,   44,  336, 1742,\n",
      "          275,  732],\n",
      "        [ 751,  751,    1,  950, 1035, 4096,    1,    1, 4970, 1049, 2689, 2689,\n",
      "          432, 3786,   11,   81, 4705,    1,    1,    1,    1, 1065,   27,  207,\n",
      "            1,   44,   30, 1295,   27,  129,    1,  139,   30,    1,   41, 1761,\n",
      "           81,  127,   27,   23,  182,  183,  309,  498,  129,   11,  341, 1097,\n",
      "           29,  277],\n",
      "        [ 182,  579,   29,  742,    1,   29, 4282,  604,   79,  736,  480, 1111,\n",
      "         1111,  130,    1,  201, 1398,   44,    1,    1, 1194,  303,   10, 1124,\n",
      "          660,   57,  182,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [1387,   90,   57,  185,  186,    1,    1,  139,  733,   57,  677,  218,\n",
      "           44, 4062,    1,    1,  202, 4174,    1,    1,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   1,    1,    1,    1,    1,  182,  183,    1,    1,  319,    1,   27,\n",
      "          470, 1097, 2158, 1848,   24,  604,   27, 2765,   46,    1, 1994,  480,\n",
      "          303, 3272,  117,   57, 5076, 1345,   27,    1, 3246,   29,    1, 3603,\n",
      "            1, 1348,   29, 2032,    1, 3242, 1503,    1,   27, 2004, 4429, 2138,\n",
      "          488,    1],\n",
      "        [ 489, 1049,  742,   19, 1050, 1051,  969,  604,   27, 1052, 1053,  114,\n",
      "          746, 1050, 1054,   41,  736,  530, 1053,  342, 1055,  109,  742,   57,\n",
      "           37,   24, 1056,  114,  406,   57, 1057,   68,  135, 1058, 1059, 1060,\n",
      "         1061, 1062, 1063, 1064,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [   1, 1263,  550,   32,   23,    1, 2582, 1957,  364, 1762, 2580, 2582,\n",
      "            1,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0]]))\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(zip(validation_loader, valid_seq_loader)):\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, v in vocab2index.items():\n",
    "#     if v==102:\n",
    "#         print (k)\n",
    "\n",
    "# vocab2index[\"कर\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, pretrained_weights, num_labels):\n",
    "        super().__init__()\n",
    "        # LSTM layers\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embeddings.weight.data.copy_(torch.from_numpy(pretrained_weights))\n",
    "        self.embeddings.weight.requires_grad = False ## freeze embeddings\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 100)\n",
    "        #self.dropout = nn.Dropout(0.2)\n",
    "        # ResNet layers\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Replace the default ResNet18 classifier layer \n",
    "        classifier_input = self.resnet.fc.in_features\n",
    "        classifier = nn.Sequential(nn.Linear(classifier_input, 100), \n",
    "                             nn.LogSoftmax(dim=1)) \n",
    "        self.resnet.fc = classifier\n",
    "        self.fusion_classifier = nn.Linear(200,2)\n",
    "        \n",
    "    def forward(self, x1, x2): # image vector, encoded image text\n",
    "        print(\"x1:\",x1)\n",
    "        print(\"x2\",x2)\n",
    "        # Image\n",
    "        x1_out = self.resnet(x1)\n",
    "        # Text\n",
    "        x2_embed = self.embeddings(x2)\n",
    "        #x2_dropout = self.dropout(x2_embed)\n",
    "        print(\"img representation:\",x1_out)\n",
    "        print(\"\")\n",
    "        lstm_out, (ht, ct) = self.lstm(x2_embed)\n",
    "        x2_out = self.linear(ht[-1])\n",
    "        print(\"text representation:\",x2_out)\n",
    "        print(\"\")\n",
    "        # Concatenation\n",
    "        fused = torch.cat([x2_out, x1_out], dim=1)\n",
    "        print(\"fused:\", fused)\n",
    "\n",
    "        return self.fusion_classifier(fused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FusionModel(vocab_size=vocab_size, embedding_dim=300, hidden_dim=5, pretrained_weights=pretrained_weights,\n",
    "                    num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, epochs, lr, train_loader, train_seq_loader):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    model.train()\n",
    "    for i in range(epochs):\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for i, data in enumerate(zip(train_loader, train_seq_loader)):\n",
    "        #for data in zip(validation_loader, valid_seq_loader): \n",
    "            img_vectors, text_sequences, y_train = data[0][0], data[1], data[0][1]      \n",
    "#             x = x.long()\n",
    "#             y = y.long()\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(img_vectors, text_sequences)\n",
    "            loss = F.cross_entropy(y_pred, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y_train.shape[0]\n",
    "            total += y_train.shape[0]\n",
    "        val_loss, val_acc = validation_metrics(model, validation_loader, valid_seq_loader)\n",
    "        print(\"train loss %.3f, val loss %.3f, val accuracy %.3f\" % (sum_loss/total, val_loss, val_acc))\n",
    "\n",
    "def validation_metrics (model, validation_loader=validation_loader, valid_seq_loader=valid_seq_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    for i, data in enumerate(zip(validation_loader, valid_seq_loader)):\n",
    "        #\n",
    "        img_vectors, text_sequences, y_valid = data[0][0], data[1], data[0][1] \n",
    "#         x = x.long()\n",
    "#         y = y.long()\n",
    "        y_hat = model(img_vectors, text_sequences)\n",
    "        loss = F.cross_entropy(y_hat, y_valid)\n",
    "        print(\"y_hat:\",y_hat)\n",
    "        pred = torch.max(y_hat, 1)[1]\n",
    "        print(\"pred:\",pred)\n",
    "        correct += (pred == y_valid).float().sum()\n",
    "        total += y_valid.shape[0]\n",
    "        sum_loss += loss.item()*y_valid.shape[0]\n",
    "    return sum_loss/total, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2884, -1.8953],\n",
      "        [-1.1243, -1.7167],\n",
      "        [-1.3596, -1.9504],\n",
      "        [-0.7396, -1.2463],\n",
      "        [-0.9927, -1.5336],\n",
      "        [-1.1708, -1.7369],\n",
      "        [-1.0991, -1.6546],\n",
      "        [-0.9892, -1.5298]], grad_fn=<AddmmBackward>)\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([[1.4987, 2.9468],\n",
      "        [1.0020, 2.0537],\n",
      "        [1.0462, 2.1332],\n",
      "        [1.3739, 2.7223],\n",
      "        [1.4643, 2.8850],\n",
      "        [0.9805, 2.0152],\n",
      "        [1.5569, 3.0513],\n",
      "        [1.6277, 3.1998]], grad_fn=<AddmmBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([[-1.4104,  1.8417],\n",
      "        [-2.2062,  2.5288],\n",
      "        [-2.1078,  2.4641],\n",
      "        [-1.9090,  2.3783],\n",
      "        [-3.5514,  3.4589],\n",
      "        [-1.8618,  2.1805],\n",
      "        [-2.8620,  2.9453],\n",
      "        [-1.9153,  2.2209]], grad_fn=<AddmmBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([[-0.7397,  3.7871],\n",
      "        [-0.8005,  4.2924],\n",
      "        [-0.7056,  3.8461],\n",
      "        [-0.6546,  3.0524],\n",
      "        [-0.8033,  4.3144],\n",
      "        [-0.6357,  3.3473],\n",
      "        [-0.5970,  2.5743],\n",
      "        [-0.8114,  4.3884]], grad_fn=<AddmmBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([[ 3.2491, -0.0924],\n",
      "        [ 3.7636, -0.0913],\n",
      "        [ 3.0767, -0.0783],\n",
      "        [ 3.4551, -0.0433],\n",
      "        [ 3.2997, -0.0923],\n",
      "        [ 3.7194, -0.1184],\n",
      "        [ 3.1366, -0.0927],\n",
      "        [ 3.3639, -0.0915]], grad_fn=<AddmmBackward>)\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([[3.1076, 3.3665],\n",
      "        [2.9885, 3.2171],\n",
      "        [2.9229, 3.1435],\n",
      "        [2.6458, 2.7746],\n",
      "        [2.9673, 3.1934],\n",
      "        [3.2854, 3.5508],\n",
      "        [2.6459, 2.7833],\n",
      "        [3.3530, 3.6109]], grad_fn=<AddmmBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([[1.8725, 0.7687],\n",
      "        [3.0440, 1.1827],\n",
      "        [2.6195, 1.0221],\n",
      "        [2.2354, 0.8919],\n",
      "        [2.0895, 0.8425],\n",
      "        [2.6177, 1.0214],\n",
      "        [2.1738, 0.8710],\n",
      "        [2.3026, 0.9147]], grad_fn=<AddmmBackward>)\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([[ 0.2996, -4.5177],\n",
      "        [ 0.2281, -3.6953],\n",
      "        [ 0.2778, -4.2672],\n",
      "        [ 0.2484, -3.9294],\n",
      "        [ 0.2122, -3.5127],\n",
      "        [ 0.1878, -3.2295],\n",
      "        [ 0.0953, -2.3754],\n",
      "        [ 0.1872, -3.1774]], grad_fn=<AddmmBackward>)\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([[1.7979, 3.0945],\n",
      "        [1.7608, 3.0028],\n",
      "        [2.0924, 3.4932],\n",
      "        [2.3150, 3.8819],\n",
      "        [2.4872, 4.0663],\n",
      "        [1.7597, 3.0010],\n",
      "        [2.1784, 3.6145],\n",
      "        [1.9223, 3.2463]], grad_fn=<AddmmBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([[-1.9439, -1.5731],\n",
      "        [-2.1798, -1.7047],\n",
      "        [-2.4968, -1.9440],\n",
      "        [-3.0068, -2.1957],\n",
      "        [-2.6307, -1.9437],\n",
      "        [-2.8776, -2.1188],\n",
      "        [-2.9640, -2.1605],\n",
      "        [-3.0625, -2.2219]], grad_fn=<AddmmBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([[-0.5302, -1.9631],\n",
      "        [-0.4607, -1.7227],\n",
      "        [-0.5402, -2.0031],\n",
      "        [-0.6628, -2.5553],\n",
      "        [-0.7081, -2.7597],\n",
      "        [-0.5223, -1.9440],\n",
      "        [-0.5130, -2.0314],\n",
      "        [-0.6469, -2.5003]], grad_fn=<AddmmBackward>)\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([[4.0986, 1.5765],\n",
      "        [3.8484, 1.4691],\n",
      "        [4.7859, 1.8713],\n",
      "        [3.4353, 1.2500],\n",
      "        [3.7087, 1.3821],\n",
      "        [3.4332, 1.2910],\n",
      "        [2.7405, 0.9744],\n",
      "        [3.6016, 1.3633]], grad_fn=<AddmmBackward>)\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([[-1.0090, -3.0735]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "train loss 1.239, val loss 1.118, val accuracy 0.546\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/Tattle/content-relevance/env/lib/python3.7/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m         \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-172-4cc2d16c1c24>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, epochs, lr, train_loader, train_seq_loader)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0msum_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_seq_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m#for data in zip(validation_loader, valid_seq_loader):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mimg_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Tattle/content-relevance/env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Tattle/content-relevance/env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Tattle/content-relevance/env/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Tattle/content-relevance/env/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Tattle/content-relevance/env/lib/python3.7/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-f014d0fe509e>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageAnnotatorClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mimg_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_image_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mimg_vector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-f014d0fe509e>\u001b[0m in \u001b[0;36mget_image_text\u001b[0;34m(self, path, client)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mimgByteArr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgByteArr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"PNG\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mimg_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgByteArr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Tattle/content-relevance/env/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2158\u001b[0;31m             \u001b[0msave_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2159\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m             \u001b[0;31m# do what we can to clean up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Tattle/content-relevance/env/lib/python3.7/site-packages/PIL/PngImagePlugin.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[0m_write_multiple_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1284\u001b[0;31m         \u001b[0mImageFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_idat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m     \u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb\"IEND\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Tattle/content-relevance/env/lib/python3.7/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m                     \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m                     \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_model(model=model, epochs=10, lr=0.01, train_loader=train_loader, train_seq_loader=train_seq_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img representation: tensor([[-4.2521, -4.2464, -4.4741, -4.5547, -4.5478, -5.5457, -5.3049, -5.1204,\n",
      "         -5.1457, -5.2056, -4.5313, -5.0253, -4.7385, -4.7661, -4.5265, -4.2586,\n",
      "         -3.3594, -4.3817, -4.1012, -5.8242, -5.5435, -5.0205, -5.0595, -4.6622,\n",
      "         -4.5196, -4.2702, -4.5995, -4.6876, -5.2176, -3.2149, -4.6365, -4.9898,\n",
      "         -5.5991, -4.2091, -4.7610, -4.1638, -3.8122, -4.7662, -5.0462, -4.9526,\n",
      "         -4.7008, -4.5671, -4.3898, -4.1583, -4.2191, -3.2409, -4.4813, -4.4752,\n",
      "         -2.9030, -4.5563, -4.8537, -4.8357, -4.2699, -4.6099, -5.8208, -5.1968,\n",
      "         -4.3736, -4.7389, -5.1744, -4.9667, -3.9274, -5.3632, -4.8872, -5.0379,\n",
      "         -5.2167, -5.8661, -4.7592, -5.0030, -5.5042, -4.4636, -4.7795, -5.1121,\n",
      "         -4.3879, -4.8093, -5.2627, -5.0186, -5.2898, -4.6608, -4.6598, -5.9072,\n",
      "         -5.5910, -4.7094, -4.7134, -5.2310, -5.1905, -4.3843, -4.2540, -4.8409,\n",
      "         -4.6072, -4.9843, -5.3077, -4.9965, -4.7092, -5.1058, -4.5060, -6.0548,\n",
      "         -5.5167, -5.2616, -5.4040, -5.0175]], grad_fn=<LogSoftmaxBackward>)\n",
      "\n",
      "text representation: tensor([[-0.3316, -0.0904,  0.1169, -0.0530,  0.1175, -0.2512,  0.2783,  0.0231,\n",
      "          0.1025, -0.3628,  0.1759, -0.4433,  0.3993,  0.0381, -0.1814, -0.3402,\n",
      "         -0.1008,  0.3423, -0.2176, -0.3097,  0.1526, -0.4573, -0.1827,  0.1051,\n",
      "         -0.0229, -0.1090,  0.2532,  0.1140,  0.3246,  0.0070, -0.4006, -0.1814,\n",
      "          0.2676, -0.3184,  0.2777,  0.2097, -0.3486, -0.2332,  0.0375,  0.1062,\n",
      "          0.1179,  0.0943, -0.3436, -0.5155, -0.1757,  0.1072, -0.3697,  0.1476,\n",
      "         -0.1967,  0.3084,  0.0094,  0.3308, -0.2688, -0.1747, -0.4722, -0.4758,\n",
      "          0.2690, -0.0164,  0.2085,  0.4478, -0.3820, -0.4052,  0.3868, -0.3084,\n",
      "          0.0341, -0.1685,  0.2567,  0.1128,  0.4728,  0.2844,  0.4709, -0.4372,\n",
      "         -0.0241, -0.4189, -0.4077,  0.4309,  0.3337, -0.0597,  0.1552, -0.3926,\n",
      "         -0.1624, -0.2317, -0.2512, -0.1209,  0.0442,  0.4739,  0.2705, -0.3573,\n",
      "         -0.0099, -0.4361,  0.3747,  0.0482,  0.2820,  0.1840,  0.2053,  0.3253,\n",
      "          0.2975,  0.2377,  0.2658,  0.1077]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "fused: tensor([[-0.3316, -0.0904,  0.1169, -0.0530,  0.1175, -0.2512,  0.2783,  0.0231,\n",
      "          0.1025, -0.3628,  0.1759, -0.4433,  0.3993,  0.0381, -0.1814, -0.3402,\n",
      "         -0.1008,  0.3423, -0.2176, -0.3097,  0.1526, -0.4573, -0.1827,  0.1051,\n",
      "         -0.0229, -0.1090,  0.2532,  0.1140,  0.3246,  0.0070, -0.4006, -0.1814,\n",
      "          0.2676, -0.3184,  0.2777,  0.2097, -0.3486, -0.2332,  0.0375,  0.1062,\n",
      "          0.1179,  0.0943, -0.3436, -0.5155, -0.1757,  0.1072, -0.3697,  0.1476,\n",
      "         -0.1967,  0.3084,  0.0094,  0.3308, -0.2688, -0.1747, -0.4722, -0.4758,\n",
      "          0.2690, -0.0164,  0.2085,  0.4478, -0.3820, -0.4052,  0.3868, -0.3084,\n",
      "          0.0341, -0.1685,  0.2567,  0.1128,  0.4728,  0.2844,  0.4709, -0.4372,\n",
      "         -0.0241, -0.4189, -0.4077,  0.4309,  0.3337, -0.0597,  0.1552, -0.3926,\n",
      "         -0.1624, -0.2317, -0.2512, -0.1209,  0.0442,  0.4739,  0.2705, -0.3573,\n",
      "         -0.0099, -0.4361,  0.3747,  0.0482,  0.2820,  0.1840,  0.2053,  0.3253,\n",
      "          0.2975,  0.2377,  0.2658,  0.1077, -4.2521, -4.2464, -4.4741, -4.5547,\n",
      "         -4.5478, -5.5457, -5.3049, -5.1204, -5.1457, -5.2056, -4.5313, -5.0253,\n",
      "         -4.7385, -4.7661, -4.5265, -4.2586, -3.3594, -4.3817, -4.1012, -5.8242,\n",
      "         -5.5435, -5.0205, -5.0595, -4.6622, -4.5196, -4.2702, -4.5995, -4.6876,\n",
      "         -5.2176, -3.2149, -4.6365, -4.9898, -5.5991, -4.2091, -4.7610, -4.1638,\n",
      "         -3.8122, -4.7662, -5.0462, -4.9526, -4.7008, -4.5671, -4.3898, -4.1583,\n",
      "         -4.2191, -3.2409, -4.4813, -4.4752, -2.9030, -4.5563, -4.8537, -4.8357,\n",
      "         -4.2699, -4.6099, -5.8208, -5.1968, -4.3736, -4.7389, -5.1744, -4.9667,\n",
      "         -3.9274, -5.3632, -4.8872, -5.0379, -5.2167, -5.8661, -4.7592, -5.0030,\n",
      "         -5.5042, -4.4636, -4.7795, -5.1121, -4.3879, -4.8093, -5.2627, -5.0186,\n",
      "         -5.2898, -4.6608, -4.6598, -5.9072, -5.5910, -4.7094, -4.7134, -5.2310,\n",
      "         -5.1905, -4.3843, -4.2540, -4.8409, -4.6072, -4.9843, -5.3077, -4.9965,\n",
      "         -4.7092, -5.1058, -4.5060, -6.0548, -5.5167, -5.2616, -5.4040, -5.0175]],\n",
      "       grad_fn=<CatBackward>)\n",
      "img representation: tensor([[-11.9913,  -3.7826, -12.5833, -11.7889, -11.2992,  -4.7991,  -4.4945,\n",
      "         -12.6855, -11.9649, -12.2276, -11.8422, -11.8447, -12.0405,  -3.7416,\n",
      "         -11.5485, -11.9999, -10.8261,  -3.6566, -12.4659,  -5.4574,  -3.5221,\n",
      "         -13.0376,  -4.1919, -12.3176,  -4.1105, -12.0193, -11.7328, -11.7507,\n",
      "          -4.4628, -11.2013, -12.0474,  -4.1130,  -5.0844, -11.2223,  -3.6835,\n",
      "          -3.1347, -11.4154, -12.1186,  -4.2668,  -4.5831, -11.7406,  -4.6845,\n",
      "          -2.9980,  -3.2289, -12.1989, -10.6418,  -4.1819, -11.4468, -10.9723,\n",
      "         -11.6317,  -4.1813, -12.5192,  -3.4268,  -3.2935,  -5.4684, -12.4832,\n",
      "          -3.8902, -11.5476,  -4.6908,  -4.4863,  -3.9324,  -4.3077, -12.5412,\n",
      "          -3.7257, -12.4882,  -4.7619, -12.2484,  -4.2403, -12.7177, -11.5256,\n",
      "          -4.2174, -12.0858,  -3.4705,  -3.7659, -12.0408,  -3.4726, -12.9526,\n",
      "          -3.7892,  -4.5419, -12.8421, -12.8729,  -3.7096, -11.8118, -12.6065,\n",
      "          -4.6220,  -3.4215,  -3.1535,  -4.5739,  -4.1453,  -4.8709, -12.8969,\n",
      "          -4.0048,  -3.8581,  -4.7366,  -3.8848, -12.9636,  -4.4669,  -5.0619,\n",
      "          -5.0294,  -3.8282],\n",
      "        [-11.8447,  -3.9464, -11.2609, -12.2781, -12.1543,  -3.9545,  -4.0019,\n",
      "         -12.2932, -11.7808, -12.0304, -13.2854, -12.1163, -12.5795,  -3.8014,\n",
      "         -12.4147, -12.2848, -11.4008,  -3.2728, -11.7993,  -4.4218,  -5.2945,\n",
      "         -12.2411,  -3.8866, -12.8508,  -4.0057, -11.6302, -11.6989, -11.8493,\n",
      "          -5.2403,  -9.8713, -11.6305,  -4.2098,  -4.4167, -10.9850,  -3.9780,\n",
      "          -3.3411, -10.9573, -12.2791,  -3.7425,  -4.7395, -11.8754,  -4.2133,\n",
      "          -3.4015,  -3.7881, -12.2334, -10.7329,  -3.8347, -11.5902, -11.0319,\n",
      "         -12.3321,  -3.6539, -12.1957,  -4.0869,  -3.7605,  -5.3315, -12.1294,\n",
      "          -4.1113, -11.4262,  -4.1386,  -4.7603,  -3.1940,  -5.0062, -11.8343,\n",
      "          -4.7427, -13.0524,  -5.2751, -12.5379,  -3.4173, -13.7172, -11.6907,\n",
      "          -3.8422, -12.7018,  -4.2619,  -4.5733, -12.8526,  -4.5814, -12.3846,\n",
      "          -4.2394,  -4.2135, -13.3160, -14.1304,  -4.3743, -11.3125, -12.6819,\n",
      "          -5.7403,  -2.4686,  -4.4718,  -3.4121,  -4.4958,  -3.9187, -12.9499,\n",
      "          -4.2822,  -3.5968,  -4.5703,  -3.0550, -13.7008,  -4.8083,  -3.8884,\n",
      "          -4.3756,  -4.7645]], grad_fn=<LogSoftmaxBackward>)\n",
      "\n",
      "text representation: tensor([[-0.2103,  0.0282,  0.1160, -0.2030,  0.2275, -0.3258,  0.3164, -0.0594,\n",
      "         -0.0302, -0.4492,  0.2578, -0.5199,  0.3643, -0.1017, -0.0760, -0.3475,\n",
      "          0.0618,  0.2056, -0.1329, -0.3692,  0.1208, -0.4269, -0.2222, -0.0311,\n",
      "          0.1317,  0.0079,  0.1631,  0.0597,  0.2512, -0.0955, -0.4269, -0.1077,\n",
      "          0.1878, -0.3261,  0.2103,  0.2665, -0.3859, -0.2376, -0.0457,  0.1387,\n",
      "          0.0695,  0.0059, -0.4196, -0.5229, -0.2515,  0.0183, -0.4664,  0.2570,\n",
      "         -0.2655,  0.3314,  0.0859,  0.2217, -0.3589, -0.1008, -0.5668, -0.3855,\n",
      "          0.3541, -0.0499,  0.0824,  0.4113, -0.3091, -0.3748,  0.3068, -0.4256,\n",
      "         -0.0429, -0.1509,  0.3975, -0.0324,  0.4924,  0.3035,  0.5576, -0.5181,\n",
      "          0.0050, -0.3126, -0.4028,  0.5135,  0.2628, -0.1475,  0.2597, -0.2604,\n",
      "         -0.2911, -0.2099, -0.3504, -0.1290,  0.1782,  0.5373,  0.3635, -0.3286,\n",
      "          0.0037, -0.3749,  0.3768,  0.0397,  0.3845,  0.2386,  0.2572,  0.2487,\n",
      "          0.3506,  0.2177,  0.3348,  0.0192],\n",
      "        [-0.3064, -0.0731,  0.1515, -0.0403,  0.1039, -0.2729,  0.2541,  0.0074,\n",
      "          0.1336, -0.3843,  0.1595, -0.4597,  0.3794,  0.0538, -0.1590, -0.3223,\n",
      "         -0.0756,  0.3078, -0.1969, -0.3080,  0.1557, -0.4400, -0.1596,  0.1113,\n",
      "          0.0119, -0.0829,  0.2829,  0.0966,  0.3460,  0.0145, -0.4161, -0.1750,\n",
      "          0.2473, -0.3320,  0.2608,  0.2286, -0.3648, -0.2163,  0.0551,  0.0862,\n",
      "          0.1023,  0.0678, -0.3626, -0.4877, -0.1713,  0.0801, -0.3612,  0.1708,\n",
      "         -0.1898,  0.3357,  0.0265,  0.3385, -0.2544, -0.1654, -0.4658, -0.4417,\n",
      "          0.2882, -0.0413,  0.2232,  0.4604, -0.3634, -0.4198,  0.4052, -0.3342,\n",
      "          0.0417, -0.1471,  0.2774,  0.0922,  0.4536,  0.3100,  0.4600, -0.4522,\n",
      "         -0.0313, -0.4304, -0.3960,  0.4074,  0.3241, -0.0349,  0.1856, -0.3849,\n",
      "         -0.1532, -0.2212, -0.2809, -0.1437,  0.0438,  0.4926,  0.2504, -0.3431,\n",
      "          0.0075, -0.4343,  0.3887,  0.0356,  0.2916,  0.2059,  0.2231,  0.3121,\n",
      "          0.3073,  0.2508,  0.2986,  0.1143]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "fused: tensor([[-2.1026e-01,  2.8228e-02,  1.1598e-01, -2.0295e-01,  2.2751e-01,\n",
      "         -3.2583e-01,  3.1641e-01, -5.9412e-02, -3.0198e-02, -4.4921e-01,\n",
      "          2.5775e-01, -5.1989e-01,  3.6434e-01, -1.0167e-01, -7.5966e-02,\n",
      "         -3.4752e-01,  6.1750e-02,  2.0556e-01, -1.3289e-01, -3.6923e-01,\n",
      "          1.2079e-01, -4.2692e-01, -2.2221e-01, -3.1149e-02,  1.3166e-01,\n",
      "          7.8980e-03,  1.6312e-01,  5.9683e-02,  2.5119e-01, -9.5521e-02,\n",
      "         -4.2688e-01, -1.0769e-01,  1.8782e-01, -3.2608e-01,  2.1032e-01,\n",
      "          2.6647e-01, -3.8592e-01, -2.3762e-01, -4.5745e-02,  1.3866e-01,\n",
      "          6.9492e-02,  5.8834e-03, -4.1962e-01, -5.2289e-01, -2.5153e-01,\n",
      "          1.8328e-02, -4.6635e-01,  2.5704e-01, -2.6550e-01,  3.3144e-01,\n",
      "          8.5872e-02,  2.2172e-01, -3.5892e-01, -1.0083e-01, -5.6681e-01,\n",
      "         -3.8555e-01,  3.5410e-01, -4.9865e-02,  8.2378e-02,  4.1126e-01,\n",
      "         -3.0912e-01, -3.7478e-01,  3.0680e-01, -4.2557e-01, -4.2914e-02,\n",
      "         -1.5094e-01,  3.9750e-01, -3.2357e-02,  4.9242e-01,  3.0353e-01,\n",
      "          5.5760e-01, -5.1812e-01,  5.0291e-03, -3.1261e-01, -4.0284e-01,\n",
      "          5.1346e-01,  2.6280e-01, -1.4754e-01,  2.5967e-01, -2.6036e-01,\n",
      "         -2.9109e-01, -2.0990e-01, -3.5036e-01, -1.2897e-01,  1.7818e-01,\n",
      "          5.3731e-01,  3.6346e-01, -3.2863e-01,  3.6723e-03, -3.7493e-01,\n",
      "          3.7682e-01,  3.9748e-02,  3.8449e-01,  2.3860e-01,  2.5715e-01,\n",
      "          2.4867e-01,  3.5055e-01,  2.1771e-01,  3.3475e-01,  1.9164e-02,\n",
      "         -1.1991e+01, -3.7826e+00, -1.2583e+01, -1.1789e+01, -1.1299e+01,\n",
      "         -4.7991e+00, -4.4945e+00, -1.2685e+01, -1.1965e+01, -1.2228e+01,\n",
      "         -1.1842e+01, -1.1845e+01, -1.2040e+01, -3.7416e+00, -1.1548e+01,\n",
      "         -1.2000e+01, -1.0826e+01, -3.6566e+00, -1.2466e+01, -5.4574e+00,\n",
      "         -3.5221e+00, -1.3038e+01, -4.1919e+00, -1.2318e+01, -4.1105e+00,\n",
      "         -1.2019e+01, -1.1733e+01, -1.1751e+01, -4.4628e+00, -1.1201e+01,\n",
      "         -1.2047e+01, -4.1130e+00, -5.0844e+00, -1.1222e+01, -3.6835e+00,\n",
      "         -3.1347e+00, -1.1415e+01, -1.2119e+01, -4.2668e+00, -4.5831e+00,\n",
      "         -1.1741e+01, -4.6845e+00, -2.9980e+00, -3.2289e+00, -1.2199e+01,\n",
      "         -1.0642e+01, -4.1819e+00, -1.1447e+01, -1.0972e+01, -1.1632e+01,\n",
      "         -4.1813e+00, -1.2519e+01, -3.4268e+00, -3.2935e+00, -5.4684e+00,\n",
      "         -1.2483e+01, -3.8902e+00, -1.1548e+01, -4.6908e+00, -4.4863e+00,\n",
      "         -3.9324e+00, -4.3077e+00, -1.2541e+01, -3.7257e+00, -1.2488e+01,\n",
      "         -4.7619e+00, -1.2248e+01, -4.2403e+00, -1.2718e+01, -1.1526e+01,\n",
      "         -4.2174e+00, -1.2086e+01, -3.4705e+00, -3.7659e+00, -1.2041e+01,\n",
      "         -3.4726e+00, -1.2953e+01, -3.7892e+00, -4.5419e+00, -1.2842e+01,\n",
      "         -1.2873e+01, -3.7096e+00, -1.1812e+01, -1.2606e+01, -4.6220e+00,\n",
      "         -3.4215e+00, -3.1535e+00, -4.5739e+00, -4.1453e+00, -4.8709e+00,\n",
      "         -1.2897e+01, -4.0048e+00, -3.8581e+00, -4.7366e+00, -3.8848e+00,\n",
      "         -1.2964e+01, -4.4669e+00, -5.0619e+00, -5.0294e+00, -3.8282e+00],\n",
      "        [-3.0637e-01, -7.3115e-02,  1.5151e-01, -4.0313e-02,  1.0391e-01,\n",
      "         -2.7288e-01,  2.5407e-01,  7.3676e-03,  1.3357e-01, -3.8432e-01,\n",
      "          1.5947e-01, -4.5970e-01,  3.7942e-01,  5.3770e-02, -1.5898e-01,\n",
      "         -3.2229e-01, -7.5596e-02,  3.0779e-01, -1.9686e-01, -3.0801e-01,\n",
      "          1.5575e-01, -4.3997e-01, -1.5958e-01,  1.1131e-01,  1.1892e-02,\n",
      "         -8.2854e-02,  2.8287e-01,  9.6609e-02,  3.4600e-01,  1.4528e-02,\n",
      "         -4.1609e-01, -1.7496e-01,  2.4727e-01, -3.3202e-01,  2.6076e-01,\n",
      "          2.2861e-01, -3.6484e-01, -2.1629e-01,  5.5140e-02,  8.6196e-02,\n",
      "          1.0226e-01,  6.7790e-02, -3.6263e-01, -4.8768e-01, -1.7135e-01,\n",
      "          8.0051e-02, -3.6123e-01,  1.7076e-01, -1.8977e-01,  3.3566e-01,\n",
      "          2.6527e-02,  3.3855e-01, -2.5443e-01, -1.6536e-01, -4.6582e-01,\n",
      "         -4.4171e-01,  2.8825e-01, -4.1344e-02,  2.2324e-01,  4.6043e-01,\n",
      "         -3.6339e-01, -4.1983e-01,  4.0520e-01, -3.3422e-01,  4.1694e-02,\n",
      "         -1.4709e-01,  2.7744e-01,  9.2178e-02,  4.5364e-01,  3.0998e-01,\n",
      "          4.5999e-01, -4.5217e-01, -3.1254e-02, -4.3045e-01, -3.9602e-01,\n",
      "          4.0738e-01,  3.2405e-01, -3.4936e-02,  1.8557e-01, -3.8488e-01,\n",
      "         -1.5318e-01, -2.2119e-01, -2.8086e-01, -1.4372e-01,  4.3834e-02,\n",
      "          4.9259e-01,  2.5044e-01, -3.4313e-01,  7.4635e-03, -4.3426e-01,\n",
      "          3.8869e-01,  3.5596e-02,  2.9161e-01,  2.0591e-01,  2.2310e-01,\n",
      "          3.1212e-01,  3.0726e-01,  2.5077e-01,  2.9863e-01,  1.1432e-01,\n",
      "         -1.1845e+01, -3.9464e+00, -1.1261e+01, -1.2278e+01, -1.2154e+01,\n",
      "         -3.9545e+00, -4.0019e+00, -1.2293e+01, -1.1781e+01, -1.2030e+01,\n",
      "         -1.3285e+01, -1.2116e+01, -1.2579e+01, -3.8014e+00, -1.2415e+01,\n",
      "         -1.2285e+01, -1.1401e+01, -3.2728e+00, -1.1799e+01, -4.4218e+00,\n",
      "         -5.2945e+00, -1.2241e+01, -3.8866e+00, -1.2851e+01, -4.0057e+00,\n",
      "         -1.1630e+01, -1.1699e+01, -1.1849e+01, -5.2403e+00, -9.8713e+00,\n",
      "         -1.1630e+01, -4.2098e+00, -4.4167e+00, -1.0985e+01, -3.9780e+00,\n",
      "         -3.3411e+00, -1.0957e+01, -1.2279e+01, -3.7425e+00, -4.7395e+00,\n",
      "         -1.1875e+01, -4.2133e+00, -3.4015e+00, -3.7881e+00, -1.2233e+01,\n",
      "         -1.0733e+01, -3.8347e+00, -1.1590e+01, -1.1032e+01, -1.2332e+01,\n",
      "         -3.6539e+00, -1.2196e+01, -4.0869e+00, -3.7605e+00, -5.3315e+00,\n",
      "         -1.2129e+01, -4.1113e+00, -1.1426e+01, -4.1386e+00, -4.7603e+00,\n",
      "         -3.1940e+00, -5.0062e+00, -1.1834e+01, -4.7427e+00, -1.3052e+01,\n",
      "         -5.2751e+00, -1.2538e+01, -3.4173e+00, -1.3717e+01, -1.1691e+01,\n",
      "         -3.8422e+00, -1.2702e+01, -4.2619e+00, -4.5733e+00, -1.2853e+01,\n",
      "         -4.5814e+00, -1.2385e+01, -4.2394e+00, -4.2135e+00, -1.3316e+01,\n",
      "         -1.4130e+01, -4.3743e+00, -1.1312e+01, -1.2682e+01, -5.7403e+00,\n",
      "         -2.4686e+00, -4.4718e+00, -3.4121e+00, -4.4958e+00, -3.9187e+00,\n",
      "         -1.2950e+01, -4.2822e+00, -3.5968e+00, -4.5703e+00, -3.0550e+00,\n",
      "         -1.3701e+01, -4.8083e+00, -3.8884e+00, -4.3756e+00, -4.7645e+00]],\n",
      "       grad_fn=<CatBackward>)\n",
      "y_hat: tensor([[ 13.5489, -14.5043],\n",
      "        [ 13.9271, -14.3401]], grad_fn=<AddmmBackward>)\n",
      "pred: tensor([0, 0])\n",
      "train loss 4.121, val loss 0.000, val accuracy 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img representation: tensor([[-11.8634,  -3.5502, -12.5868, -11.5374, -11.5819,  -4.5397,  -3.7741,\n",
      "         -12.0026, -12.0417, -12.6410, -11.9186, -12.4750, -12.8020,  -4.2791,\n",
      "         -11.7074, -11.7172, -10.8717,  -3.9249, -11.0223,  -5.0255,  -4.0409,\n",
      "         -12.3385,  -4.9010, -11.6947,  -3.9779, -10.9718, -11.3072, -11.6124,\n",
      "          -4.0879, -10.6153, -11.6457,  -4.1957,  -4.7426, -12.2339,  -4.7658,\n",
      "          -3.5958, -11.3617, -11.6555,  -3.8889,  -4.5319, -12.5150,  -4.3914,\n",
      "          -4.3513,  -3.6762, -10.7070, -10.1089,  -3.0840, -10.9793, -10.0362,\n",
      "         -11.9951,  -4.2718, -12.1493,  -4.2256,  -3.4949,  -3.9752, -11.8257,\n",
      "          -3.2105, -11.7752,  -4.6601,  -4.5970,  -2.3097,  -4.9880, -11.9044,\n",
      "          -4.8332, -12.7361,  -5.1199, -11.5616,  -4.3642, -12.2760, -11.3185,\n",
      "          -3.7257, -12.6843,  -3.3394,  -4.4739, -13.2273,  -5.1117, -12.5062,\n",
      "          -4.4007,  -4.1046, -11.7557, -11.9558,  -4.0044, -11.1070, -12.5791,\n",
      "          -4.1102,  -4.4747,  -3.6217,  -3.6227,  -3.7704,  -3.7628, -12.1570,\n",
      "          -4.2027,  -4.2135,  -3.4227,  -4.1878, -12.4906,  -5.3745,  -3.8763,\n",
      "          -5.1955,  -3.9700]], grad_fn=<LogSoftmaxBackward>)\n",
      "\n",
      "text representation: tensor([[-0.3064, -0.0731,  0.1515, -0.0403,  0.1039, -0.2729,  0.2541,  0.0074,\n",
      "          0.1336, -0.3843,  0.1595, -0.4597,  0.3794,  0.0538, -0.1590, -0.3223,\n",
      "         -0.0756,  0.3078, -0.1969, -0.3080,  0.1557, -0.4400, -0.1596,  0.1113,\n",
      "          0.0119, -0.0829,  0.2829,  0.0966,  0.3460,  0.0145, -0.4161, -0.1750,\n",
      "          0.2473, -0.3320,  0.2608,  0.2286, -0.3648, -0.2163,  0.0551,  0.0862,\n",
      "          0.1023,  0.0678, -0.3626, -0.4877, -0.1713,  0.0801, -0.3612,  0.1708,\n",
      "         -0.1898,  0.3357,  0.0265,  0.3385, -0.2544, -0.1654, -0.4658, -0.4417,\n",
      "          0.2882, -0.0413,  0.2232,  0.4604, -0.3634, -0.4198,  0.4052, -0.3342,\n",
      "          0.0417, -0.1471,  0.2774,  0.0922,  0.4536,  0.3100,  0.4600, -0.4522,\n",
      "         -0.0313, -0.4304, -0.3960,  0.4074,  0.3241, -0.0349,  0.1856, -0.3849,\n",
      "         -0.1532, -0.2212, -0.2809, -0.1437,  0.0438,  0.4926,  0.2504, -0.3431,\n",
      "          0.0075, -0.4343,  0.3887,  0.0356,  0.2916,  0.2059,  0.2231,  0.3121,\n",
      "          0.3073,  0.2508,  0.2986,  0.1143]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "fused: tensor([[-3.0637e-01, -7.3115e-02,  1.5151e-01, -4.0313e-02,  1.0391e-01,\n",
      "         -2.7288e-01,  2.5407e-01,  7.3676e-03,  1.3357e-01, -3.8432e-01,\n",
      "          1.5947e-01, -4.5970e-01,  3.7942e-01,  5.3770e-02, -1.5898e-01,\n",
      "         -3.2229e-01, -7.5596e-02,  3.0779e-01, -1.9686e-01, -3.0801e-01,\n",
      "          1.5575e-01, -4.3997e-01, -1.5958e-01,  1.1131e-01,  1.1892e-02,\n",
      "         -8.2854e-02,  2.8287e-01,  9.6609e-02,  3.4600e-01,  1.4528e-02,\n",
      "         -4.1609e-01, -1.7496e-01,  2.4727e-01, -3.3202e-01,  2.6076e-01,\n",
      "          2.2861e-01, -3.6484e-01, -2.1629e-01,  5.5140e-02,  8.6196e-02,\n",
      "          1.0226e-01,  6.7790e-02, -3.6263e-01, -4.8768e-01, -1.7135e-01,\n",
      "          8.0051e-02, -3.6123e-01,  1.7076e-01, -1.8977e-01,  3.3566e-01,\n",
      "          2.6527e-02,  3.3855e-01, -2.5443e-01, -1.6536e-01, -4.6582e-01,\n",
      "         -4.4171e-01,  2.8825e-01, -4.1344e-02,  2.2324e-01,  4.6043e-01,\n",
      "         -3.6339e-01, -4.1983e-01,  4.0520e-01, -3.3422e-01,  4.1694e-02,\n",
      "         -1.4709e-01,  2.7744e-01,  9.2178e-02,  4.5364e-01,  3.0998e-01,\n",
      "          4.5999e-01, -4.5217e-01, -3.1254e-02, -4.3045e-01, -3.9602e-01,\n",
      "          4.0738e-01,  3.2405e-01, -3.4936e-02,  1.8557e-01, -3.8488e-01,\n",
      "         -1.5318e-01, -2.2119e-01, -2.8086e-01, -1.4372e-01,  4.3834e-02,\n",
      "          4.9259e-01,  2.5044e-01, -3.4313e-01,  7.4635e-03, -4.3426e-01,\n",
      "          3.8869e-01,  3.5596e-02,  2.9161e-01,  2.0591e-01,  2.2310e-01,\n",
      "          3.1212e-01,  3.0726e-01,  2.5077e-01,  2.9863e-01,  1.1432e-01,\n",
      "         -1.1863e+01, -3.5502e+00, -1.2587e+01, -1.1537e+01, -1.1582e+01,\n",
      "         -4.5397e+00, -3.7741e+00, -1.2003e+01, -1.2042e+01, -1.2641e+01,\n",
      "         -1.1919e+01, -1.2475e+01, -1.2802e+01, -4.2791e+00, -1.1707e+01,\n",
      "         -1.1717e+01, -1.0872e+01, -3.9249e+00, -1.1022e+01, -5.0255e+00,\n",
      "         -4.0409e+00, -1.2339e+01, -4.9010e+00, -1.1695e+01, -3.9779e+00,\n",
      "         -1.0972e+01, -1.1307e+01, -1.1612e+01, -4.0879e+00, -1.0615e+01,\n",
      "         -1.1646e+01, -4.1957e+00, -4.7426e+00, -1.2234e+01, -4.7658e+00,\n",
      "         -3.5958e+00, -1.1362e+01, -1.1656e+01, -3.8889e+00, -4.5319e+00,\n",
      "         -1.2515e+01, -4.3914e+00, -4.3513e+00, -3.6762e+00, -1.0707e+01,\n",
      "         -1.0109e+01, -3.0840e+00, -1.0979e+01, -1.0036e+01, -1.1995e+01,\n",
      "         -4.2718e+00, -1.2149e+01, -4.2256e+00, -3.4949e+00, -3.9752e+00,\n",
      "         -1.1826e+01, -3.2105e+00, -1.1775e+01, -4.6601e+00, -4.5970e+00,\n",
      "         -2.3097e+00, -4.9880e+00, -1.1904e+01, -4.8332e+00, -1.2736e+01,\n",
      "         -5.1199e+00, -1.1562e+01, -4.3642e+00, -1.2276e+01, -1.1318e+01,\n",
      "         -3.7257e+00, -1.2684e+01, -3.3394e+00, -4.4739e+00, -1.3227e+01,\n",
      "         -5.1117e+00, -1.2506e+01, -4.4007e+00, -4.1046e+00, -1.1756e+01,\n",
      "         -1.1956e+01, -4.0044e+00, -1.1107e+01, -1.2579e+01, -4.1102e+00,\n",
      "         -4.4747e+00, -3.6217e+00, -3.6227e+00, -3.7704e+00, -3.7628e+00,\n",
      "         -1.2157e+01, -4.2027e+00, -4.2135e+00, -3.4227e+00, -4.1878e+00,\n",
      "         -1.2491e+01, -5.3745e+00, -3.8763e+00, -5.1955e+00, -3.9700e+00]],\n",
      "       grad_fn=<CatBackward>)\n",
      "img representation: tensor([[-17.4329,  -3.7823, -18.0250, -17.2305, -16.7409,  -4.7988,  -4.4942,\n",
      "         -18.1271, -17.4065, -17.6692, -17.2838, -17.2863, -17.4821,  -3.7412,\n",
      "         -16.9901, -17.4415, -16.2678,  -3.6563, -17.9076,  -5.4571,  -3.5218,\n",
      "         -18.4792,  -4.1915, -17.7592,  -4.1102, -17.4609, -17.1744, -17.1923,\n",
      "          -4.4624, -16.6429, -17.4891,  -4.1126,  -5.0841, -16.6639,  -3.6831,\n",
      "          -3.1343, -16.8570, -17.5602,  -4.2664,  -4.5828, -17.1822,  -4.6842,\n",
      "          -2.9977,  -3.2286, -17.6405, -16.0835,  -4.1815, -16.8885, -16.4139,\n",
      "         -17.0733,  -4.1809, -17.9608,  -3.4265,  -3.2931,  -5.4681, -17.9248,\n",
      "          -3.8898, -16.9892,  -4.6905,  -4.4860,  -3.9320,  -4.3074, -17.9829,\n",
      "          -3.7254, -17.9298,  -4.7616, -17.6900,  -4.2400, -18.1593, -16.9672,\n",
      "          -4.2171, -17.5274,  -3.4701,  -3.7656, -17.4824,  -3.4723, -18.3942,\n",
      "          -3.7889,  -4.5415, -18.2837, -18.3145,  -3.7093, -17.2535, -18.0481,\n",
      "          -4.6217,  -3.4212,  -3.1531,  -4.5736,  -4.1450,  -4.8706, -18.3385,\n",
      "          -4.0044,  -3.8578,  -4.7362,  -3.8844, -18.4052,  -4.4666,  -5.0616,\n",
      "          -5.0290,  -3.8278],\n",
      "        [-17.2941,  -3.9461, -16.7102, -17.7275, -17.6036,  -3.9542,  -4.0016,\n",
      "         -17.7426, -17.2302, -17.4797, -18.7348, -17.5656, -18.0288,  -3.8010,\n",
      "         -17.8640, -17.7341, -16.8502,  -3.2725, -17.2487,  -4.4215,  -5.2942,\n",
      "         -17.6904,  -3.8863, -18.3002,  -4.0054, -17.0796, -17.1482, -17.2987,\n",
      "          -5.2400, -15.3207, -17.0798,  -4.2095,  -4.4163, -16.4343,  -3.9776,\n",
      "          -3.3407, -16.4067, -17.7284,  -3.7422,  -4.7391, -17.3247,  -4.2130,\n",
      "          -3.4011,  -3.7878, -17.6828, -16.1822,  -3.8344, -17.0395, -16.4813,\n",
      "         -17.7814,  -3.6535, -17.6450,  -4.0866,  -3.7601,  -5.3311, -17.5787,\n",
      "          -4.1109, -16.8756,  -4.1382,  -4.7599,  -3.1937,  -5.0059, -17.2836,\n",
      "          -4.7423, -18.5018,  -5.2747, -17.9872,  -3.4170, -19.1666, -17.1400,\n",
      "          -3.8418, -18.1512,  -4.2615,  -4.5730, -18.3019,  -4.5811, -17.8339,\n",
      "          -4.2390,  -4.2131, -18.7654, -19.5797,  -4.3739, -16.7618, -18.1313,\n",
      "          -5.7400,  -2.4683,  -4.4714,  -3.4117,  -4.4955,  -3.9184, -18.3993,\n",
      "          -4.2819,  -3.5964,  -4.5700,  -3.0546, -19.1501,  -4.8080,  -3.8881,\n",
      "          -4.3752,  -4.7641]], grad_fn=<LogSoftmaxBackward>)\n",
      "\n",
      "text representation: tensor([[-1.9149e-01,  4.1578e-02,  1.3939e-01, -1.9426e-01,  2.1661e-01,\n",
      "         -3.4219e-01,  2.9929e-01, -7.2414e-02, -9.3311e-03, -4.6360e-01,\n",
      "          2.4490e-01, -5.3257e-01,  3.4906e-01, -9.1103e-02, -5.9155e-02,\n",
      "         -3.3520e-01,  8.0963e-02,  1.8181e-01, -1.1728e-01, -3.6601e-01,\n",
      "          1.2532e-01, -4.1395e-01, -2.0582e-01, -2.5627e-02,  1.5630e-01,\n",
      "          2.7490e-02,  1.8295e-01,  4.7108e-02,  2.6662e-01, -8.9922e-02,\n",
      "         -4.3843e-01, -1.0229e-01,  1.7174e-01, -3.3785e-01,  1.9728e-01,\n",
      "          2.7953e-01, -3.9825e-01, -2.2568e-01, -3.1980e-02,  1.2343e-01,\n",
      "          5.6846e-02, -1.2437e-02, -4.3383e-01, -5.0335e-01, -2.4746e-01,\n",
      "         -5.9381e-05, -4.5904e-01,  2.7425e-01, -2.5969e-01,  3.5011e-01,\n",
      "          9.8861e-02,  2.2778e-01, -3.4836e-01, -9.2460e-02, -5.6068e-01,\n",
      "         -3.6262e-01,  3.6802e-01, -6.8127e-02,  9.2642e-02,  4.2185e-01,\n",
      "         -2.9398e-01, -3.8559e-01,  3.2054e-01, -4.4533e-01, -3.6169e-02,\n",
      "         -1.3432e-01,  4.1474e-01, -4.8133e-02,  4.7737e-01,  3.2240e-01,\n",
      "          5.4916e-01, -5.2999e-01, -2.7375e-03, -3.2125e-01, -3.9253e-01,\n",
      "          4.9640e-01,  2.5437e-01, -1.3056e-01,  2.8152e-01, -2.5193e-01,\n",
      "         -2.8488e-01, -2.0013e-01, -3.7114e-01, -1.4524e-01,  1.7605e-01,\n",
      "          5.5086e-01,  3.4995e-01, -3.1729e-01,  1.7288e-02, -3.7611e-01,\n",
      "          3.8674e-01,  2.9740e-02,  3.9208e-01,  2.5380e-01,  2.7076e-01,\n",
      "          2.3850e-01,  3.5910e-01,  2.2785e-01,  3.5793e-01,  2.5274e-02],\n",
      "        [-2.8937e-01, -6.1495e-02,  1.7470e-01, -3.1954e-02,  9.4706e-02,\n",
      "         -2.8764e-01,  2.3796e-01, -3.5850e-03,  1.5433e-01, -3.9889e-01,\n",
      "          1.4872e-01, -4.7103e-01,  3.6634e-01,  6.3957e-02, -1.4340e-01,\n",
      "         -3.1044e-01, -5.8238e-02,  2.8474e-01, -1.8238e-01, -3.0692e-01,\n",
      "          1.5789e-01, -4.2833e-01, -1.4413e-01,  1.1567e-01,  3.5313e-02,\n",
      "         -6.5375e-02,  3.0270e-01,  8.4887e-02,  3.6031e-01,  1.9222e-02,\n",
      "         -4.2656e-01, -1.7045e-01,  2.3365e-01, -3.4109e-01,  2.4936e-01,\n",
      "          2.4127e-01, -3.7548e-01, -2.0488e-01,  6.7064e-02,  7.2999e-02,\n",
      "          9.2020e-02,  5.0090e-02, -3.7514e-01, -4.6917e-01, -1.6866e-01,\n",
      "          6.1702e-02, -3.5586e-01,  1.8653e-01, -1.8509e-01,  3.5392e-01,\n",
      "          3.8145e-02,  3.4363e-01, -2.4513e-01, -1.5900e-01, -4.6174e-01,\n",
      "         -4.1895e-01,  3.0089e-01, -5.7952e-02,  2.3316e-01,  4.6922e-01,\n",
      "         -3.5082e-01, -4.2980e-01,  4.1756e-01, -3.5160e-01,  4.6472e-02,\n",
      "         -1.3241e-01,  2.9167e-01,  7.8591e-02,  4.4118e-01,  3.2709e-01,\n",
      "          4.5284e-01, -4.6261e-01, -3.6188e-02, -4.3840e-01, -3.8793e-01,\n",
      "          3.9178e-01,  3.1756e-01, -1.8244e-02,  2.0601e-01, -3.7965e-01,\n",
      "         -1.4721e-01, -2.1413e-01, -3.0068e-01, -1.5904e-01,  4.3302e-02,\n",
      "          5.0500e-01,  2.3686e-01, -3.3354e-01,  1.9014e-02, -4.3299e-01,\n",
      "          3.9791e-01,  2.6786e-02,  2.9829e-01,  2.2066e-01,  2.3526e-01,\n",
      "          3.0324e-01,  3.1349e-01,  2.5996e-01,  3.2071e-01,  1.1859e-01]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "\n",
      "fused: tensor([[-1.9149e-01,  4.1578e-02,  1.3939e-01, -1.9426e-01,  2.1661e-01,\n",
      "         -3.4219e-01,  2.9929e-01, -7.2414e-02, -9.3311e-03, -4.6360e-01,\n",
      "          2.4490e-01, -5.3257e-01,  3.4906e-01, -9.1103e-02, -5.9155e-02,\n",
      "         -3.3520e-01,  8.0963e-02,  1.8181e-01, -1.1728e-01, -3.6601e-01,\n",
      "          1.2532e-01, -4.1395e-01, -2.0582e-01, -2.5627e-02,  1.5630e-01,\n",
      "          2.7490e-02,  1.8295e-01,  4.7108e-02,  2.6662e-01, -8.9922e-02,\n",
      "         -4.3843e-01, -1.0229e-01,  1.7174e-01, -3.3785e-01,  1.9728e-01,\n",
      "          2.7953e-01, -3.9825e-01, -2.2568e-01, -3.1980e-02,  1.2343e-01,\n",
      "          5.6846e-02, -1.2437e-02, -4.3383e-01, -5.0335e-01, -2.4746e-01,\n",
      "         -5.9381e-05, -4.5904e-01,  2.7425e-01, -2.5969e-01,  3.5011e-01,\n",
      "          9.8861e-02,  2.2778e-01, -3.4836e-01, -9.2460e-02, -5.6068e-01,\n",
      "         -3.6262e-01,  3.6802e-01, -6.8127e-02,  9.2642e-02,  4.2185e-01,\n",
      "         -2.9398e-01, -3.8559e-01,  3.2054e-01, -4.4533e-01, -3.6169e-02,\n",
      "         -1.3432e-01,  4.1474e-01, -4.8133e-02,  4.7737e-01,  3.2240e-01,\n",
      "          5.4916e-01, -5.2999e-01, -2.7375e-03, -3.2125e-01, -3.9253e-01,\n",
      "          4.9640e-01,  2.5437e-01, -1.3056e-01,  2.8152e-01, -2.5193e-01,\n",
      "         -2.8488e-01, -2.0013e-01, -3.7114e-01, -1.4524e-01,  1.7605e-01,\n",
      "          5.5086e-01,  3.4995e-01, -3.1729e-01,  1.7288e-02, -3.7611e-01,\n",
      "          3.8674e-01,  2.9740e-02,  3.9208e-01,  2.5380e-01,  2.7076e-01,\n",
      "          2.3850e-01,  3.5910e-01,  2.2785e-01,  3.5793e-01,  2.5274e-02,\n",
      "         -1.7433e+01, -3.7823e+00, -1.8025e+01, -1.7231e+01, -1.6741e+01,\n",
      "         -4.7988e+00, -4.4942e+00, -1.8127e+01, -1.7407e+01, -1.7669e+01,\n",
      "         -1.7284e+01, -1.7286e+01, -1.7482e+01, -3.7412e+00, -1.6990e+01,\n",
      "         -1.7442e+01, -1.6268e+01, -3.6563e+00, -1.7908e+01, -5.4571e+00,\n",
      "         -3.5218e+00, -1.8479e+01, -4.1915e+00, -1.7759e+01, -4.1102e+00,\n",
      "         -1.7461e+01, -1.7174e+01, -1.7192e+01, -4.4624e+00, -1.6643e+01,\n",
      "         -1.7489e+01, -4.1126e+00, -5.0841e+00, -1.6664e+01, -3.6831e+00,\n",
      "         -3.1343e+00, -1.6857e+01, -1.7560e+01, -4.2664e+00, -4.5828e+00,\n",
      "         -1.7182e+01, -4.6842e+00, -2.9977e+00, -3.2286e+00, -1.7640e+01,\n",
      "         -1.6083e+01, -4.1815e+00, -1.6888e+01, -1.6414e+01, -1.7073e+01,\n",
      "         -4.1809e+00, -1.7961e+01, -3.4265e+00, -3.2931e+00, -5.4681e+00,\n",
      "         -1.7925e+01, -3.8898e+00, -1.6989e+01, -4.6905e+00, -4.4860e+00,\n",
      "         -3.9320e+00, -4.3074e+00, -1.7983e+01, -3.7254e+00, -1.7930e+01,\n",
      "         -4.7616e+00, -1.7690e+01, -4.2400e+00, -1.8159e+01, -1.6967e+01,\n",
      "         -4.2171e+00, -1.7527e+01, -3.4701e+00, -3.7656e+00, -1.7482e+01,\n",
      "         -3.4723e+00, -1.8394e+01, -3.7889e+00, -4.5415e+00, -1.8284e+01,\n",
      "         -1.8315e+01, -3.7093e+00, -1.7253e+01, -1.8048e+01, -4.6217e+00,\n",
      "         -3.4212e+00, -3.1531e+00, -4.5736e+00, -4.1450e+00, -4.8706e+00,\n",
      "         -1.8339e+01, -4.0044e+00, -3.8578e+00, -4.7362e+00, -3.8844e+00,\n",
      "         -1.8405e+01, -4.4666e+00, -5.0616e+00, -5.0290e+00, -3.8278e+00],\n",
      "        [-2.8937e-01, -6.1495e-02,  1.7470e-01, -3.1954e-02,  9.4706e-02,\n",
      "         -2.8764e-01,  2.3796e-01, -3.5850e-03,  1.5433e-01, -3.9889e-01,\n",
      "          1.4872e-01, -4.7103e-01,  3.6634e-01,  6.3957e-02, -1.4340e-01,\n",
      "         -3.1044e-01, -5.8238e-02,  2.8474e-01, -1.8238e-01, -3.0692e-01,\n",
      "          1.5789e-01, -4.2833e-01, -1.4413e-01,  1.1567e-01,  3.5313e-02,\n",
      "         -6.5375e-02,  3.0270e-01,  8.4887e-02,  3.6031e-01,  1.9222e-02,\n",
      "         -4.2656e-01, -1.7045e-01,  2.3365e-01, -3.4109e-01,  2.4936e-01,\n",
      "          2.4127e-01, -3.7548e-01, -2.0488e-01,  6.7064e-02,  7.2999e-02,\n",
      "          9.2020e-02,  5.0090e-02, -3.7514e-01, -4.6917e-01, -1.6866e-01,\n",
      "          6.1702e-02, -3.5586e-01,  1.8653e-01, -1.8509e-01,  3.5392e-01,\n",
      "          3.8145e-02,  3.4363e-01, -2.4513e-01, -1.5900e-01, -4.6174e-01,\n",
      "         -4.1895e-01,  3.0089e-01, -5.7952e-02,  2.3316e-01,  4.6922e-01,\n",
      "         -3.5082e-01, -4.2980e-01,  4.1756e-01, -3.5160e-01,  4.6472e-02,\n",
      "         -1.3241e-01,  2.9167e-01,  7.8591e-02,  4.4118e-01,  3.2709e-01,\n",
      "          4.5284e-01, -4.6261e-01, -3.6188e-02, -4.3840e-01, -3.8793e-01,\n",
      "          3.9178e-01,  3.1756e-01, -1.8244e-02,  2.0601e-01, -3.7965e-01,\n",
      "         -1.4721e-01, -2.1413e-01, -3.0068e-01, -1.5904e-01,  4.3302e-02,\n",
      "          5.0500e-01,  2.3686e-01, -3.3354e-01,  1.9014e-02, -4.3299e-01,\n",
      "          3.9791e-01,  2.6786e-02,  2.9829e-01,  2.2066e-01,  2.3526e-01,\n",
      "          3.0324e-01,  3.1349e-01,  2.5996e-01,  3.2071e-01,  1.1859e-01,\n",
      "         -1.7294e+01, -3.9461e+00, -1.6710e+01, -1.7727e+01, -1.7604e+01,\n",
      "         -3.9542e+00, -4.0016e+00, -1.7743e+01, -1.7230e+01, -1.7480e+01,\n",
      "         -1.8735e+01, -1.7566e+01, -1.8029e+01, -3.8010e+00, -1.7864e+01,\n",
      "         -1.7734e+01, -1.6850e+01, -3.2725e+00, -1.7249e+01, -4.4215e+00,\n",
      "         -5.2942e+00, -1.7690e+01, -3.8863e+00, -1.8300e+01, -4.0054e+00,\n",
      "         -1.7080e+01, -1.7148e+01, -1.7299e+01, -5.2400e+00, -1.5321e+01,\n",
      "         -1.7080e+01, -4.2095e+00, -4.4163e+00, -1.6434e+01, -3.9776e+00,\n",
      "         -3.3407e+00, -1.6407e+01, -1.7728e+01, -3.7422e+00, -4.7391e+00,\n",
      "         -1.7325e+01, -4.2130e+00, -3.4011e+00, -3.7878e+00, -1.7683e+01,\n",
      "         -1.6182e+01, -3.8344e+00, -1.7040e+01, -1.6481e+01, -1.7781e+01,\n",
      "         -3.6535e+00, -1.7645e+01, -4.0866e+00, -3.7601e+00, -5.3311e+00,\n",
      "         -1.7579e+01, -4.1109e+00, -1.6876e+01, -4.1382e+00, -4.7599e+00,\n",
      "         -3.1937e+00, -5.0059e+00, -1.7284e+01, -4.7423e+00, -1.8502e+01,\n",
      "         -5.2747e+00, -1.7987e+01, -3.4170e+00, -1.9167e+01, -1.7140e+01,\n",
      "         -3.8418e+00, -1.8151e+01, -4.2615e+00, -4.5730e+00, -1.8302e+01,\n",
      "         -4.5811e+00, -1.7834e+01, -4.2390e+00, -4.2131e+00, -1.8765e+01,\n",
      "         -1.9580e+01, -4.3739e+00, -1.6762e+01, -1.8131e+01, -5.7400e+00,\n",
      "         -2.4683e+00, -4.4714e+00, -3.4117e+00, -4.4955e+00, -3.9184e+00,\n",
      "         -1.8399e+01, -4.2819e+00, -3.5964e+00, -4.5700e+00, -3.0546e+00,\n",
      "         -1.9150e+01, -4.8080e+00, -3.8881e+00, -4.3752e+00, -4.7641e+00]],\n",
      "       grad_fn=<CatBackward>)\n",
      "y_hat: tensor([[ 28.1080, -29.6945],\n",
      "        [ 28.5386, -29.5849]], grad_fn=<AddmmBackward>)\n",
      "pred: tensor([0, 0])\n",
      "train loss 0.000, val loss 0.000, val accuracy 1.000\n",
      "CPU times: user 3.38 s, sys: 163 ms, total: 3.54 s\n",
      "Wall time: 17.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_model(model=model, epochs=2, lr=0.01, train_loader=train_loader, train_seq_loader=train_seq_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.Tensor(([[-4.2521, -4.2464, -4.4741, -4.5547, -4.5478, -5.5457, -5.3049, -5.1204,\n",
    "         -5.1457, -5.2056, -4.5313, -5.0253, -4.7385, -4.7661, -4.5265, -4.2586,\n",
    "         -3.3594, -4.3817, -4.1012, -5.8242, -5.5435, -5.0205, -5.0595, -4.6622,\n",
    "         -4.5196, -4.2702, -4.5995, -4.6876, -5.2176, -3.2149, -4.6365, -4.9898,\n",
    "         -5.5991, -4.2091, -4.7610, -4.1638, -3.8122, -4.7662, -5.0462, -4.9526,\n",
    "         -4.7008, -4.5671, -4.3898, -4.1583, -4.2191, -3.2409, -4.4813, -4.4752,\n",
    "         -2.9030, -4.5563, -4.8537, -4.8357, -4.2699, -4.6099, -5.8208, -5.1968,\n",
    "         -4.3736, -4.7389, -5.1744, -4.9667, -3.9274, -5.3632, -4.8872, -5.0379,\n",
    "         -5.2167, -5.8661, -4.7592, -5.0030, -5.5042, -4.4636, -4.7795, -5.1121,\n",
    "         -4.3879, -4.8093, -5.2627, -5.0186, -5.2898, -4.6608, -4.6598, -5.9072,\n",
    "         -5.5910, -4.7094, -4.7134, -5.2310, -5.1905, -4.3843, -4.2540, -4.8409,\n",
    "         -4.6072, -4.9843, -5.3077, -4.9965, -4.7092, -5.1058, -4.5060, -6.0548,\n",
    "         -5.5167, -5.2616, -5.4040, -5.0175]])\n",
    "               \n",
    "b=torch.Tensor(([[-11.9913,  -3.7826, -12.5833, -11.7889, -11.2992,  -4.7991,  -4.4945,\n",
    "         -12.6855, -11.9649, -12.2276, -11.8422, -11.8447, -12.0405,  -3.7416,\n",
    "         -11.5485, -11.9999, -10.8261,  -3.6566, -12.4659,  -5.4574,  -3.5221,\n",
    "         -13.0376,  -4.1919, -12.3176,  -4.1105, -12.0193, -11.7328, -11.7507,\n",
    "          -4.4628, -11.2013, -12.0474,  -4.1130,  -5.0844, -11.2223,  -3.6835,\n",
    "          -3.1347, -11.4154, -12.1186,  -4.2668,  -4.5831, -11.7406,  -4.6845,\n",
    "          -2.9980,  -3.2289, -12.1989, -10.6418,  -4.1819, -11.4468, -10.9723,\n",
    "         -11.6317,  -4.1813, -12.5192,  -3.4268,  -3.2935,  -5.4684, -12.4832,\n",
    "          -3.8902, -11.5476,  -4.6908,  -4.4863,  -3.9324,  -4.3077, -12.5412,\n",
    "          -3.7257, -12.4882,  -4.7619, -12.2484,  -4.2403, -12.7177, -11.5256,\n",
    "          -4.2174, -12.0858,  -3.4705,  -3.7659, -12.0408,  -3.4726, -12.9526,\n",
    "          -3.7892,  -4.5419, -12.8421, -12.8729,  -3.7096, -11.8118, -12.6065,\n",
    "          -4.6220,  -3.4215,  -3.1535,  -4.5739,  -4.1453,  -4.8709, -12.8969,\n",
    "          -4.0048,  -3.8581,  -4.7366,  -3.8848, -12.9636,  -4.4669,  -5.0619,\n",
    "          -5.0294,  -3.8282],\n",
    "        [-11.8447,  -3.9464, -11.2609, -12.2781, -12.1543,  -3.9545,  -4.0019,\n",
    "         -12.2932, -11.7808, -12.0304, -13.2854, -12.1163, -12.5795,  -3.8014,\n",
    "         -12.4147, -12.2848, -11.4008,  -3.2728, -11.7993,  -4.4218,  -5.2945,\n",
    "         -12.2411,  -3.8866, -12.8508,  -4.0057, -11.6302, -11.6989, -11.8493,\n",
    "          -5.2403,  -9.8713, -11.6305,  -4.2098,  -4.4167, -10.9850,  -3.9780,\n",
    "          -3.3411, -10.9573, -12.2791,  -3.7425,  -4.7395, -11.8754,  -4.2133,\n",
    "          -3.4015,  -3.7881, -12.2334, -10.7329,  -3.8347, -11.5902, -11.0319,\n",
    "         -12.3321,  -3.6539, -12.1957,  -4.0869,  -3.7605,  -5.3315, -12.1294,\n",
    "          -4.1113, -11.4262,  -4.1386,  -4.7603,  -3.1940,  -5.0062, -11.8343,\n",
    "          -4.7427, -13.0524,  -5.2751, -12.5379,  -3.4173, -13.7172, -11.6907,\n",
    "          -3.8422, -12.7018,  -4.2619,  -4.5733, -12.8526,  -4.5814, -12.3846,\n",
    "          -4.2394,  -4.2135, -13.3160, -14.1304,  -4.3743, -11.3125, -12.6819,\n",
    "          -5.7403,  -2.4686,  -4.4718,  -3.4121,  -4.4958,  -3.9187, -12.9499,\n",
    "          -4.2822,  -3.5968,  -4.5703,  -3.0550, -13.7008,  -4.8083,  -3.8884,\n",
    "          -4.3756,  -4.7645]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]])\n"
     ]
    }
   ],
   "source": [
    "for i in train_seq_loader:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: tensor([[[[-2.0323, -2.0323, -2.0323,  ..., -2.0152, -2.0152, -2.0323],\n",
      "          [-2.0323, -2.0323, -2.0323,  ..., -1.9467, -1.9638, -2.0323],\n",
      "          [-2.0323, -2.0323, -2.0323,  ..., -1.9124, -1.9124, -1.9980],\n",
      "          ...,\n",
      "          [-1.5870, -1.5870, -1.5870,  ..., -1.3644, -1.3473, -1.3473],\n",
      "          [-1.5699, -1.5699, -1.5699,  ..., -1.3644, -1.3644, -1.3815],\n",
      "          [-1.5357, -1.5357, -1.5357,  ..., -1.3644, -1.3815, -1.4158]],\n",
      "\n",
      "         [[-0.6877, -0.6877, -0.6877,  ..., -0.7052, -0.6877, -0.6702],\n",
      "          [-0.6877, -0.6877, -0.6877,  ..., -0.6702, -0.6702, -0.6702],\n",
      "          [-0.6877, -0.6877, -0.6877,  ..., -0.7227, -0.6527, -0.6352],\n",
      "          ...,\n",
      "          [-0.7402, -0.7402, -0.7402,  ..., -0.3550, -0.3375, -0.3375],\n",
      "          [-0.7227, -0.7227, -0.7227,  ..., -0.3550, -0.3550, -0.3725],\n",
      "          [-0.6877, -0.6877, -0.6877,  ..., -0.3550, -0.3725, -0.4076]],\n",
      "\n",
      "         [[ 0.8274,  0.8274,  0.8274,  ...,  0.7054,  0.7228,  0.7576],\n",
      "          [ 0.8274,  0.8274,  0.8274,  ...,  0.6705,  0.6879,  0.7576],\n",
      "          [ 0.8274,  0.8274,  0.8274,  ...,  0.5659,  0.6531,  0.7925],\n",
      "          ...,\n",
      "          [ 0.2696,  0.2696,  0.2696,  ...,  0.6879,  0.7054,  0.7054],\n",
      "          [ 0.2871,  0.2871,  0.2871,  ...,  0.6879,  0.6879,  0.6705],\n",
      "          [ 0.3219,  0.3219,  0.3219,  ...,  0.6879,  0.6705,  0.6356]]]])\n",
      "x2 tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]])\n",
      "img representation: tensor([[-4.0983, -5.2199, -4.3792, -4.2309, -6.0392, -4.8014, -4.3715, -6.4910,\n",
      "         -5.2845, -4.2545, -5.0993, -5.1253, -4.4658, -4.7478, -4.6579, -4.8724,\n",
      "         -5.4594, -4.2422, -4.4594, -5.5135, -3.6493, -5.6196, -4.9082, -5.1369,\n",
      "         -6.3514, -5.2456, -5.2697, -5.2253, -4.7091, -5.7350, -5.5734, -5.1303,\n",
      "         -4.2368, -5.0091, -5.3515, -3.9397, -5.0215, -4.4803, -4.7826, -4.7912,\n",
      "         -4.1746, -4.2604, -6.2648, -3.3710, -4.7571, -4.9140, -4.9384, -4.6549,\n",
      "         -4.9208, -3.5884, -5.6102, -5.4464, -4.8741, -5.4224, -4.9623, -5.4987,\n",
      "         -4.0284, -4.9247, -6.2442, -4.6860, -4.2235, -3.9102, -4.2861, -5.6787,\n",
      "         -4.8862, -4.7575, -3.8320, -4.7272, -4.5979, -4.4000, -4.9106, -4.0333,\n",
      "         -5.3516, -4.4798, -5.5390, -4.9156, -4.7305, -3.3986, -4.7312, -4.4670,\n",
      "         -4.9871, -4.4925, -5.4017, -4.8602, -4.5116, -5.4511, -4.0252, -3.0386,\n",
      "         -4.5819, -4.2542, -5.0201, -4.8838, -4.1832, -4.9835, -4.6042, -4.6242,\n",
      "         -4.6148, -4.5404, -5.5486, -4.8036]], grad_fn=<LogSoftmaxBackward>)\n",
      "\n",
      "text representation: tensor([[ 0.1639, -0.3111, -0.4007,  0.5131, -0.2668, -0.3702, -0.4471, -0.0204,\n",
      "          0.1470, -0.3462, -0.0126, -0.2149, -0.2513,  0.1541,  0.3976,  0.1780,\n",
      "          0.3718,  0.4206, -0.1505, -0.2242,  0.2860,  0.1610, -0.1429, -0.3934,\n",
      "          0.3989,  0.0188, -0.0967,  0.3527, -0.3924,  0.3418, -0.4307, -0.0292,\n",
      "          0.3275,  0.0539,  0.3745, -0.2629,  0.3123, -0.3041, -0.4015, -0.0666,\n",
      "          0.0118,  0.3639, -0.3430,  0.2979, -0.0953,  0.3415, -0.0453,  0.1580,\n",
      "          0.1798, -0.1467, -0.0181,  0.2003,  0.2404,  0.0718,  0.1840,  0.1125,\n",
      "          0.4157,  0.0774,  0.3057, -0.0996,  0.0024, -0.2145,  0.4077, -0.1666,\n",
      "         -0.3298,  0.3600, -0.3502, -0.0478, -0.3832,  0.1605,  0.3129,  0.3739,\n",
      "         -0.2646, -0.1075, -0.3411,  0.1096, -0.3657, -0.2992, -0.1551,  0.3694,\n",
      "         -0.0734, -0.0488, -0.0988,  0.2925, -0.2129, -0.1392,  0.3759,  0.3700,\n",
      "         -0.1510,  0.3445, -0.3728,  0.2721, -0.2267,  0.1782, -0.1369,  0.0494,\n",
      "          0.2306, -0.3084,  0.1122,  0.0037]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "fused: tensor([[ 1.6391e-01, -3.1114e-01, -4.0067e-01,  5.1315e-01, -2.6678e-01,\n",
      "         -3.7022e-01, -4.4706e-01, -2.0450e-02,  1.4698e-01, -3.4616e-01,\n",
      "         -1.2638e-02, -2.1490e-01, -2.5128e-01,  1.5412e-01,  3.9764e-01,\n",
      "          1.7797e-01,  3.7181e-01,  4.2064e-01, -1.5051e-01, -2.2422e-01,\n",
      "          2.8605e-01,  1.6096e-01, -1.4288e-01, -3.9342e-01,  3.9894e-01,\n",
      "          1.8798e-02, -9.6714e-02,  3.5273e-01, -3.9239e-01,  3.4185e-01,\n",
      "         -4.3066e-01, -2.9174e-02,  3.2752e-01,  5.3901e-02,  3.7453e-01,\n",
      "         -2.6292e-01,  3.1229e-01, -3.0405e-01, -4.0147e-01, -6.6642e-02,\n",
      "          1.1818e-02,  3.6392e-01, -3.4299e-01,  2.9789e-01, -9.5260e-02,\n",
      "          3.4150e-01, -4.5275e-02,  1.5798e-01,  1.7981e-01, -1.4675e-01,\n",
      "         -1.8102e-02,  2.0029e-01,  2.4037e-01,  7.1760e-02,  1.8399e-01,\n",
      "          1.1246e-01,  4.1575e-01,  7.7428e-02,  3.0567e-01, -9.9643e-02,\n",
      "          2.3701e-03, -2.1455e-01,  4.0773e-01, -1.6655e-01, -3.2981e-01,\n",
      "          3.6000e-01, -3.5018e-01, -4.7760e-02, -3.8320e-01,  1.6053e-01,\n",
      "          3.1292e-01,  3.7389e-01, -2.6461e-01, -1.0748e-01, -3.4108e-01,\n",
      "          1.0965e-01, -3.6568e-01, -2.9925e-01, -1.5510e-01,  3.6936e-01,\n",
      "         -7.3433e-02, -4.8833e-02, -9.8835e-02,  2.9248e-01, -2.1294e-01,\n",
      "         -1.3916e-01,  3.7594e-01,  3.7003e-01, -1.5102e-01,  3.4450e-01,\n",
      "         -3.7279e-01,  2.7210e-01, -2.2668e-01,  1.7819e-01, -1.3690e-01,\n",
      "          4.9450e-02,  2.3057e-01, -3.0840e-01,  1.1219e-01,  3.7428e-03,\n",
      "         -4.0983e+00, -5.2199e+00, -4.3792e+00, -4.2309e+00, -6.0392e+00,\n",
      "         -4.8014e+00, -4.3715e+00, -6.4910e+00, -5.2845e+00, -4.2545e+00,\n",
      "         -5.0993e+00, -5.1253e+00, -4.4658e+00, -4.7478e+00, -4.6579e+00,\n",
      "         -4.8724e+00, -5.4594e+00, -4.2422e+00, -4.4594e+00, -5.5135e+00,\n",
      "         -3.6493e+00, -5.6196e+00, -4.9082e+00, -5.1369e+00, -6.3514e+00,\n",
      "         -5.2456e+00, -5.2697e+00, -5.2253e+00, -4.7091e+00, -5.7350e+00,\n",
      "         -5.5734e+00, -5.1303e+00, -4.2368e+00, -5.0091e+00, -5.3515e+00,\n",
      "         -3.9397e+00, -5.0215e+00, -4.4803e+00, -4.7826e+00, -4.7912e+00,\n",
      "         -4.1746e+00, -4.2604e+00, -6.2648e+00, -3.3710e+00, -4.7571e+00,\n",
      "         -4.9140e+00, -4.9384e+00, -4.6549e+00, -4.9208e+00, -3.5884e+00,\n",
      "         -5.6102e+00, -5.4464e+00, -4.8741e+00, -5.4224e+00, -4.9623e+00,\n",
      "         -5.4987e+00, -4.0284e+00, -4.9247e+00, -6.2442e+00, -4.6860e+00,\n",
      "         -4.2235e+00, -3.9102e+00, -4.2861e+00, -5.6787e+00, -4.8862e+00,\n",
      "         -4.7575e+00, -3.8320e+00, -4.7272e+00, -4.5979e+00, -4.4000e+00,\n",
      "         -4.9106e+00, -4.0333e+00, -5.3516e+00, -4.4798e+00, -5.5390e+00,\n",
      "         -4.9156e+00, -4.7305e+00, -3.3986e+00, -4.7312e+00, -4.4670e+00,\n",
      "         -4.9871e+00, -4.4925e+00, -5.4017e+00, -4.8602e+00, -4.5116e+00,\n",
      "         -5.4511e+00, -4.0252e+00, -3.0386e+00, -4.5819e+00, -4.2542e+00,\n",
      "         -5.0201e+00, -4.8838e+00, -4.1832e+00, -4.9835e+00, -4.6042e+00,\n",
      "         -4.6242e+00, -4.6148e+00, -4.5404e+00, -5.5486e+00, -4.8036e+00]],\n",
      "       grad_fn=<CatBackward>)\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(zip(train_loader, train_seq_loader)):\n",
    "        #for data in zip(validation_loader, valid_seq_loader): \n",
    "    img_vectors, text_sequences, y_train = data[0][0], data[1], data[0][1]      \n",
    "#             x = x.long()\n",
    "#             y = y.long()\n",
    "    #optimizer.zero_grad()\n",
    "    y_pred = model(img_vectors, text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1595852529951,
   "trusted": true
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
