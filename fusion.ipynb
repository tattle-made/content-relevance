{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import os\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from io import BytesIO\n",
    "import captum\n",
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import NoiseTunnel\n",
    "from torchsummary import summary\n",
    "import wget\n",
    "import sqlite3, json, io\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from pprint import pprint\n",
    "import json\n",
    "import math\n",
    "from math import sqrt\n",
    "import torch\n",
    "import itertools \n",
    "import torchtext\n",
    "from torchtext.data import Field, Dataset, Example\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from google.cloud import vision\n",
    "import sys,os,json\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from google.cloud import vision\n",
    "from google.protobuf.json_format import MessageToJson\n",
    "import requests\n",
    "import logging\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "from torch.utils.data import DataLoader\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import io\n",
    "from io import BytesIO\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wget.download(\"https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.hi.align.vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images and image text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations=transforms.Compose([\n",
    "                    transforms.Resize((224,224)), \n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, txt_path, img_dir, \n",
    "                 transform):\n",
    "        df = pd.read_csv(txt_path, sep=\" \", header=None, names= [\"img_name\"])\n",
    "        df[\"label\"] = df[\"img_name\"].apply(lambda x: 0 if \"neg\" in x else 1)\n",
    "        self.img_dir=img_dir\n",
    "        self.txt_path=txt_path\n",
    "        self.img_names=df[\"img_name\"].values\n",
    "        self.y = df[\"label\"].values\n",
    "        self.transform=transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(os.path.join(self.img_dir, self.img_names[idx])).convert('RGB')\n",
    "        client = vision.ImageAnnotatorClient()\n",
    "        img_text = self.get_image_text(os.path.join(self.img_dir, self.img_names[idx]), client)\n",
    "        if self.transform is not None:\n",
    "            img_vector=self.transform(img)\n",
    "        \n",
    "        label = self.y[idx]\n",
    "        return img_vector,label,img_text\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "    \n",
    "    def get_image_text(self, path, client):  \n",
    "        def detect_text(img_bytes, client):\n",
    "            image_data = vision.types.Image(content=img_bytes)\n",
    "            resp = client.text_detection(image=image_data)\n",
    "            resp = json.loads(MessageToJson(resp))\n",
    "            text = resp.get('fullTextAnnotation',{}).get(\"text\",\"\")\n",
    "            return text\n",
    "        \n",
    "        image = Image.open(path, mode=\"r\")\n",
    "        imgByteArr = io.BytesIO()\n",
    "        image.save(imgByteArr, format=\"PNG\")\n",
    "        img_bytes = imgByteArr.getvalue()\n",
    "        text = detect_text(img_bytes, client)\n",
    "        return text\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = CustomDataset(txt_path=\"/Users/kruttikanadig/Documents/Tattle/machine-learning/textfile.txt\",\n",
    "#                        img_dir=\"/Users/kruttikanadig/Documents/Tattle/machine-learning/all\",\n",
    "#                        transform=transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(txt_path=\"/Users/kruttikanadig/Documents/Tattle/machine-learning/temp.txt\",\n",
    "                       img_dir=\"/Users/kruttikanadig/Documents/Tattle/machine-learning/few\",\n",
    "                       transform=transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create text preprocessing pipeline\n",
    "\n",
    "1. Tokenize training data and get word counts\n",
    "2. Load aligned word embeddings into aligned dict \n",
    "2. Create vocab and vocab2index using aligned dict and word counts\n",
    "3. Encode training data using vocab and vocab2index \n",
    "4. Apply to validation / test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(dataloader, inference=False, path=None, vocab2index=None):\n",
    "    \"\"\" Encodes text into sequences of vocabulary indices \"\"\"\n",
    "    \n",
    "    def tokenize(text):\n",
    "        \"\"\" Removes punctuation and numbers, converts to lowercase and splits into individual words \"\"\"\n",
    "        regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n\\ред]') # last character ред is the Hindi full stop\n",
    "        nopunct = regex.sub(\" \", text.lower())\n",
    "        tokenized_text = [token for token in nopunct.split(\" \") if len(token) > 0]\n",
    "        return tokenized_text\n",
    "    \n",
    "    def count_words(tokens, counts):\n",
    "        \"\"\" Counts unique words in text \"\"\"\n",
    "        counts.update(list(chain.from_iterable(tokens)))\n",
    "        return counts\n",
    "    \n",
    "    def load_embeddings(path):\n",
    "        \"\"\" Loads pretrained aligned word embeddings \"\"\"\n",
    "        words = []\n",
    "        idx = 0\n",
    "        word2idx = {}\n",
    "        vectors = []\n",
    "        fin = io.open(path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "        n, d = map(int, fin.readline().split())\n",
    "        for line in tqdm.tqdm(fin,total=n):\n",
    "            tokens = line.rstrip().split(' ')\n",
    "            vec = (list(map(float, tokens[1:])))\n",
    "            word = tokens[0].replace(\"'\",'\"')\n",
    "            words.append(word)\n",
    "            word2idx[word]=idx\n",
    "            vectors.append(vec)\n",
    "            idx+=1\n",
    "\n",
    "        vectors = np.array(vectors)\n",
    "        aligned_dict = {w: vectors[word2idx[w]] for w in words}\n",
    "        return aligned_dict\n",
    "    \n",
    "    def get_emb_matrix(pretrained, word_counts, emb_size = 300):\n",
    "        \"\"\" Creates training vocabulary, vocab2index and embedding matrix from pretrained word vectors \"\"\"\n",
    "        found=0\n",
    "        not_found=0\n",
    "        vocab_size = len(word_counts) + 2\n",
    "        vocab_to_idx = {}\n",
    "        vocab = [\"\", \"UNK\"]\n",
    "        W = np.zeros((vocab_size, emb_size), dtype=\"float32\")\n",
    "        W[0] = np.zeros(emb_size, dtype='float32') # adding a vector for padding\n",
    "        W[1] = np.random.uniform(-0.25, 0.25, emb_size) # adding a vector for unknown words \n",
    "        vocab_to_idx[\"UNK\"] = 1\n",
    "        i = 2\n",
    "        for word in word_counts:\n",
    "            if word in pretrained:\n",
    "                W[i] = pretrained[word]\n",
    "                found+=1\n",
    "            else:\n",
    "                W[i] = np.random.uniform(-0.25,0.25, emb_size)\n",
    "                not_found+=1\n",
    "            vocab_to_idx[word] = i\n",
    "            vocab.append(word)\n",
    "            i += 1   \n",
    "        return W, np.array(vocab), vocab_to_idx, found, not_found\n",
    "    \n",
    "    def encode_tokens(tokenized_text, vocab2index, N=50): # keep max doc length to 50 words\n",
    "        \"\"\" Encodes tokenized text into equal length sequences of vocabulary indices \"\"\" \n",
    "        sequences = []\n",
    "        for text in tokenized_text:\n",
    "            seq = np.zeros(N, dtype=int)\n",
    "            # get word index if it's in vocab else get default index\n",
    "            enc = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in text])\n",
    "            # limit sequence length\n",
    "            length = min(N, len(enc))\n",
    "            seq[:length] = enc[:length]\n",
    "            sequences.append(seq)\n",
    "        return sequences\n",
    "\n",
    "    if inference==False:\n",
    "        counts = Counter()\n",
    "        tokenized_text = []\n",
    "        print(\"Tokenizing text and counting unique words\")\n",
    "        for i in dataloader:\n",
    "            tokenized = [tokenize(doc) for doc in i[2]]\n",
    "            counts = count_words(tokenized, counts)\n",
    "            tokenized_text.extend(tokenized)\n",
    "\n",
    "        print(\"Loading pretrained aligned word embeddings\")\n",
    "        aligned_dict = load_embeddings(path)\n",
    "\n",
    "        print(\"Creating vocab2index and embedding matrix with pretrained weights\")\n",
    "        pretrained_weights, vocab, vocab2index, found, not_found = get_emb_matrix(aligned_dict, counts)\n",
    "        sequences = encode_tokens(tokenized_text, vocab2index)\n",
    "        vocab_size = len(vocab)\n",
    "        print(\"Finished preparing sequences\")\n",
    "        return sequences, vocab, vocab_size, vocab2index, pretrained_weights\n",
    "    else:\n",
    "        print(\"Tokenizing text\")\n",
    "        tokenized_text = []\n",
    "        for i in dataloader:\n",
    "            tokenized = [tokenize(doc) for doc in i[2]]\n",
    "            tokenized_text.extend(tokenized)\n",
    "        sequences = encode_tokens(tokenized_text, vocab2index)\n",
    "        print(\"Finished preparing sequences\")\n",
    "        return sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and randomly split dataset into 80:20 training and validation sets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "valid_size = len(dataset) - train_size\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=False)\n",
    "validation_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing text and counting unique words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|тЦП         | 1979/158016 [00:00<00:15, 9922.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained aligned word embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 158016/158016 [00:19<00:00, 8131.42it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocab2index and embedding matrix with pretrained weights\n",
      "Finished preparing sequences\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/kruttikanadig/Documents/Tattle/machine-learning/wiki.hi.align.vec\"\n",
    "train_sequences, vocab, vocab_size, vocab2index, pretrained_weights = prepare_sequences(train_loader, inference=False, path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing text\n",
      "Finished preparing sequences\n"
     ]
    }
   ],
   "source": [
    "valid_sequences = prepare_sequences(validation_loader, inference=True, vocab2index=vocab2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_loader = torch.utils.data.DataLoader(train_sequences, batch_size=2, shuffle=False)\n",
    "valid_seq_loader = torch.utils.data.DataLoader(valid_sequences, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, v in vocab2index.items():\n",
    "#     if v==102:\n",
    "#         print (k)\n",
    "\n",
    "# vocab2index[\"рдХрд░\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, pretrained_weights, num_labels):\n",
    "        super().__init__()\n",
    "        # LSTM layers\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embeddings.weight.data.copy_(torch.from_numpy(pretrained_weights))\n",
    "        self.embeddings.weight.requires_grad = False ## freeze embeddings\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 100)\n",
    "        #self.dropout = nn.Dropout(0.2)\n",
    "        # ResNet layers\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Replace the default ResNet18 classifier layer \n",
    "        classifier_input = self.resnet.fc.in_features\n",
    "        classifier = nn.Sequential(nn.Linear(classifier_input, 100), \n",
    "                             nn.LogSoftmax(dim=1)) \n",
    "        self.resnet.fc = classifier\n",
    "        self.fusion_classifier = nn.Linear(200,2)\n",
    "        \n",
    "    def forward(self, x1, x2): # image vector, encoded image text\n",
    "        print(\"x1:\",x1)\n",
    "        print(\"x2\",x2)\n",
    "        # Image\n",
    "        x1_out = self.resnet(x1)\n",
    "        # Text\n",
    "        x2_embed = self.embeddings(x2)\n",
    "        #x2_dropout = self.dropout(x2_embed)\n",
    "        print(\"img representation:\",x1_out)\n",
    "        print(\"\")\n",
    "        lstm_out, (ht, ct) = self.lstm(x2_embed)\n",
    "        x2_out = self.linear(ht[-1])\n",
    "        print(\"text representation:\",x2_out)\n",
    "        print(\"\")\n",
    "        # Concatenation\n",
    "        fused = torch.cat([x2_out, x1_out], dim=1)\n",
    "        print(\"fused:\", fused)\n",
    "\n",
    "        return self.fusion_classifier(fused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FusionModel(vocab_size=vocab_size, embedding_dim=300, hidden_dim=5, pretrained_weights=pretrained_weights,\n",
    "                    num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, epochs, lr, train_loader, train_seq_loader):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    model.train()\n",
    "    for i in range(epochs):\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for i, data in enumerate(zip(train_loader, train_seq_loader)):\n",
    "        #for data in zip(validation_loader, valid_seq_loader): \n",
    "            img_vectors, text_sequences, y_train = data[0][0], data[1], data[0][1]      \n",
    "#             x = x.long()\n",
    "#             y = y.long()\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(img_vectors, text_sequences)\n",
    "            loss = F.cross_entropy(y_pred, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y_train.shape[0]\n",
    "            total += y_train.shape[0]\n",
    "        val_loss, val_acc = validation_metrics(model, validation_loader, valid_seq_loader)\n",
    "        print(\"train loss %.3f, val loss %.3f, val accuracy %.3f\" % (sum_loss/total, val_loss, val_acc))\n",
    "\n",
    "def validation_metrics (model, validation_loader=validation_loader, valid_seq_loader=valid_seq_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    for i, data in enumerate(zip(validation_loader, valid_seq_loader)):\n",
    "        #\n",
    "        img_vectors, text_sequences, y_valid = data[0][0], data[1], data[0][1] \n",
    "#         x = x.long()\n",
    "#         y = y.long()\n",
    "        y_hat = model(img_vectors, text_sequences)\n",
    "        loss = F.cross_entropy(y_hat, y_valid)\n",
    "        print(\"y_hat:\",y_hat)\n",
    "        pred = torch.max(y_hat, 1)[1]\n",
    "        print(\"pred:\",pred)\n",
    "        correct += (pred == y_valid).float().sum()\n",
    "        total += y_valid.shape[0]\n",
    "        sum_loss += loss.item()*y_valid.shape[0]\n",
    "    return sum_loss/total, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2884, -1.8953],\n",
      "        [-1.1243, -1.7167],\n",
      "        [-1.3596, -1.9504],\n",
      "        [-0.7396, -1.2463],\n",
      "        [-0.9927, -1.5336],\n",
      "        [-1.1708, -1.7369],\n",
      "        [-1.0991, -1.6546],\n",
      "        [-0.9892, -1.5298]], grad_fn=<AddmmBackward>)\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([[1.4987, 2.9468],\n",
      "        [1.0020, 2.0537],\n",
      "        [1.0462, 2.1332],\n",
      "        [1.3739, 2.7223],\n",
      "        [1.4643, 2.8850],\n",
      "        [0.9805, 2.0152],\n",
      "        [1.5569, 3.0513],\n",
      "        [1.6277, 3.1998]], grad_fn=<AddmmBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([[-1.4104,  1.8417],\n",
      "        [-2.2062,  2.5288],\n",
      "        [-2.1078,  2.4641],\n",
      "        [-1.9090,  2.3783],\n",
      "        [-3.5514,  3.4589],\n",
      "        [-1.8618,  2.1805],\n",
      "        [-2.8620,  2.9453],\n",
      "        [-1.9153,  2.2209]], grad_fn=<AddmmBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([[-0.7397,  3.7871],\n",
      "        [-0.8005,  4.2924],\n",
      "        [-0.7056,  3.8461],\n",
      "        [-0.6546,  3.0524],\n",
      "        [-0.8033,  4.3144],\n",
      "        [-0.6357,  3.3473],\n",
      "        [-0.5970,  2.5743],\n",
      "        [-0.8114,  4.3884]], grad_fn=<AddmmBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([[ 3.2491, -0.0924],\n",
      "        [ 3.7636, -0.0913],\n",
      "        [ 3.0767, -0.0783],\n",
      "        [ 3.4551, -0.0433],\n",
      "        [ 3.2997, -0.0923],\n",
      "        [ 3.7194, -0.1184],\n",
      "        [ 3.1366, -0.0927],\n",
      "        [ 3.3639, -0.0915]], grad_fn=<AddmmBackward>)\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([[3.1076, 3.3665],\n",
      "        [2.9885, 3.2171],\n",
      "        [2.9229, 3.1435],\n",
      "        [2.6458, 2.7746],\n",
      "        [2.9673, 3.1934],\n",
      "        [3.2854, 3.5508],\n",
      "        [2.6459, 2.7833],\n",
      "        [3.3530, 3.6109]], grad_fn=<AddmmBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([[1.8725, 0.7687],\n",
      "        [3.0440, 1.1827],\n",
      "        [2.6195, 1.0221],\n",
      "        [2.2354, 0.8919],\n",
      "        [2.0895, 0.8425],\n",
      "        [2.6177, 1.0214],\n",
      "        [2.1738, 0.8710],\n",
      "        [2.3026, 0.9147]], grad_fn=<AddmmBackward>)\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([[ 0.2996, -4.5177],\n",
      "        [ 0.2281, -3.6953],\n",
      "        [ 0.2778, -4.2672],\n",
      "        [ 0.2484, -3.9294],\n",
      "        [ 0.2122, -3.5127],\n",
      "        [ 0.1878, -3.2295],\n",
      "        [ 0.0953, -2.3754],\n",
      "        [ 0.1872, -3.1774]], grad_fn=<AddmmBackward>)\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([[1.7979, 3.0945],\n",
      "        [1.7608, 3.0028],\n",
      "        [2.0924, 3.4932],\n",
      "        [2.3150, 3.8819],\n",
      "        [2.4872, 4.0663],\n",
      "        [1.7597, 3.0010],\n",
      "        [2.1784, 3.6145],\n",
      "        [1.9223, 3.2463]], grad_fn=<AddmmBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([[-1.9439, -1.5731],\n",
      "        [-2.1798, -1.7047],\n",
      "        [-2.4968, -1.9440],\n",
      "        [-3.0068, -2.1957],\n",
      "        [-2.6307, -1.9437],\n",
      "        [-2.8776, -2.1188],\n",
      "        [-2.9640, -2.1605],\n",
      "        [-3.0625, -2.2219]], grad_fn=<AddmmBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([[-0.5302, -1.9631],\n",
      "        [-0.4607, -1.7227],\n",
      "        [-0.5402, -2.0031],\n",
      "        [-0.6628, -2.5553],\n",
      "        [-0.7081, -2.7597],\n",
      "        [-0.5223, -1.9440],\n",
      "        [-0.5130, -2.0314],\n",
      "        [-0.6469, -2.5003]], grad_fn=<AddmmBackward>)\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([[4.0986, 1.5765],\n",
      "        [3.8484, 1.4691],\n",
      "        [4.7859, 1.8713],\n",
      "        [3.4353, 1.2500],\n",
      "        [3.7087, 1.3821],\n",
      "        [3.4332, 1.2910],\n",
      "        [2.7405, 0.9744],\n",
      "        [3.6016, 1.3633]], grad_fn=<AddmmBackward>)\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([[-1.0090, -3.0735]], grad_fn=<AddmmBackward>)\n",
      "tensor([0])\n",
      "train loss 1.239, val loss 1.118, val accuracy 0.546\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/Tattle/content-relevance/env/lib/python3.7/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m         \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-172-4cc2d16c1c24>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, epochs, lr, train_loader, train_seq_loader)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0msum_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_seq_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m#for data in zip(validation_loader, valid_seq_loader):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mimg_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Tattle/content-relevance/env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Tattle/content-relevance/env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Tattle/content-relevance/env/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Tattle/content-relevance/env/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Tattle/content-relevance/env/lib/python3.7/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-f014d0fe509e>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageAnnotatorClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mimg_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_image_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mimg_vector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-f014d0fe509e>\u001b[0m in \u001b[0;36mget_image_text\u001b[0;34m(self, path, client)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mimgByteArr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgByteArr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"PNG\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mimg_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgByteArr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Tattle/content-relevance/env/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2158\u001b[0;31m             \u001b[0msave_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2159\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m             \u001b[0;31m# do what we can to clean up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Tattle/content-relevance/env/lib/python3.7/site-packages/PIL/PngImagePlugin.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[0m_write_multiple_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1284\u001b[0;31m         \u001b[0mImageFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_idat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m     \u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb\"IEND\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Tattle/content-relevance/env/lib/python3.7/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m                     \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m                     \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_model(model=model, epochs=10, lr=0.01, train_loader=train_loader, train_seq_loader=train_seq_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# train_model(model=model, epochs=2, lr=0.01, train_loader=train_loader, train_seq_loader=train_seq_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.Tensor(([[-4.2521, -4.2464, -4.4741, -4.5547, -4.5478, -5.5457, -5.3049, -5.1204,\n",
    "         -5.1457, -5.2056, -4.5313, -5.0253, -4.7385, -4.7661, -4.5265, -4.2586,\n",
    "         -3.3594, -4.3817, -4.1012, -5.8242, -5.5435, -5.0205, -5.0595, -4.6622,\n",
    "         -4.5196, -4.2702, -4.5995, -4.6876, -5.2176, -3.2149, -4.6365, -4.9898,\n",
    "         -5.5991, -4.2091, -4.7610, -4.1638, -3.8122, -4.7662, -5.0462, -4.9526,\n",
    "         -4.7008, -4.5671, -4.3898, -4.1583, -4.2191, -3.2409, -4.4813, -4.4752,\n",
    "         -2.9030, -4.5563, -4.8537, -4.8357, -4.2699, -4.6099, -5.8208, -5.1968,\n",
    "         -4.3736, -4.7389, -5.1744, -4.9667, -3.9274, -5.3632, -4.8872, -5.0379,\n",
    "         -5.2167, -5.8661, -4.7592, -5.0030, -5.5042, -4.4636, -4.7795, -5.1121,\n",
    "         -4.3879, -4.8093, -5.2627, -5.0186, -5.2898, -4.6608, -4.6598, -5.9072,\n",
    "         -5.5910, -4.7094, -4.7134, -5.2310, -5.1905, -4.3843, -4.2540, -4.8409,\n",
    "         -4.6072, -4.9843, -5.3077, -4.9965, -4.7092, -5.1058, -4.5060, -6.0548,\n",
    "         -5.5167, -5.2616, -5.4040, -5.0175]])\n",
    "               \n",
    "b=torch.Tensor(([[-11.9913,  -3.7826, -12.5833, -11.7889, -11.2992,  -4.7991,  -4.4945,\n",
    "         -12.6855, -11.9649, -12.2276, -11.8422, -11.8447, -12.0405,  -3.7416,\n",
    "         -11.5485, -11.9999, -10.8261,  -3.6566, -12.4659,  -5.4574,  -3.5221,\n",
    "         -13.0376,  -4.1919, -12.3176,  -4.1105, -12.0193, -11.7328, -11.7507,\n",
    "          -4.4628, -11.2013, -12.0474,  -4.1130,  -5.0844, -11.2223,  -3.6835,\n",
    "          -3.1347, -11.4154, -12.1186,  -4.2668,  -4.5831, -11.7406,  -4.6845,\n",
    "          -2.9980,  -3.2289, -12.1989, -10.6418,  -4.1819, -11.4468, -10.9723,\n",
    "         -11.6317,  -4.1813, -12.5192,  -3.4268,  -3.2935,  -5.4684, -12.4832,\n",
    "          -3.8902, -11.5476,  -4.6908,  -4.4863,  -3.9324,  -4.3077, -12.5412,\n",
    "          -3.7257, -12.4882,  -4.7619, -12.2484,  -4.2403, -12.7177, -11.5256,\n",
    "          -4.2174, -12.0858,  -3.4705,  -3.7659, -12.0408,  -3.4726, -12.9526,\n",
    "          -3.7892,  -4.5419, -12.8421, -12.8729,  -3.7096, -11.8118, -12.6065,\n",
    "          -4.6220,  -3.4215,  -3.1535,  -4.5739,  -4.1453,  -4.8709, -12.8969,\n",
    "          -4.0048,  -3.8581,  -4.7366,  -3.8848, -12.9636,  -4.4669,  -5.0619,\n",
    "          -5.0294,  -3.8282],\n",
    "        [-11.8447,  -3.9464, -11.2609, -12.2781, -12.1543,  -3.9545,  -4.0019,\n",
    "         -12.2932, -11.7808, -12.0304, -13.2854, -12.1163, -12.5795,  -3.8014,\n",
    "         -12.4147, -12.2848, -11.4008,  -3.2728, -11.7993,  -4.4218,  -5.2945,\n",
    "         -12.2411,  -3.8866, -12.8508,  -4.0057, -11.6302, -11.6989, -11.8493,\n",
    "          -5.2403,  -9.8713, -11.6305,  -4.2098,  -4.4167, -10.9850,  -3.9780,\n",
    "          -3.3411, -10.9573, -12.2791,  -3.7425,  -4.7395, -11.8754,  -4.2133,\n",
    "          -3.4015,  -3.7881, -12.2334, -10.7329,  -3.8347, -11.5902, -11.0319,\n",
    "         -12.3321,  -3.6539, -12.1957,  -4.0869,  -3.7605,  -5.3315, -12.1294,\n",
    "          -4.1113, -11.4262,  -4.1386,  -4.7603,  -3.1940,  -5.0062, -11.8343,\n",
    "          -4.7427, -13.0524,  -5.2751, -12.5379,  -3.4173, -13.7172, -11.6907,\n",
    "          -3.8422, -12.7018,  -4.2619,  -4.5733, -12.8526,  -4.5814, -12.3846,\n",
    "          -4.2394,  -4.2135, -13.3160, -14.1304,  -4.3743, -11.3125, -12.6819,\n",
    "          -5.7403,  -2.4686,  -4.4718,  -3.4121,  -4.4958,  -3.9187, -12.9499,\n",
    "          -4.2822,  -3.5968,  -4.5703,  -3.0550, -13.7008,  -4.8083,  -3.8884,\n",
    "          -4.3756,  -4.7645]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, data in enumerate(zip(train_loader, train_seq_loader)):\n",
    "#     img_vectors, text_sequences, y_train = data[0][0], data[1], data[0][1]      \n",
    "#     y_pred = model(img_vectors, text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1595852529951,
   "trusted": true
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
